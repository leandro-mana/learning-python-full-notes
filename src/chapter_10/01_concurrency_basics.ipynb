{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Concurrency Basics\n",
    "\n",
    "**Chapter 10 - Learning Python, 5th Edition**\n",
    "\n",
    "Python provides multiple concurrency models: **threading** for I/O-bound parallelism,\n",
    "**multiprocessing** for CPU-bound parallelism, and **asyncio** for cooperative\n",
    "multitasking. Understanding the Global Interpreter Lock (GIL) and when each model\n",
    "applies is essential for writing performant Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Threading Basics\n",
    "\n",
    "The `threading` module provides a high-level interface for creating and managing\n",
    "threads. Threads share the same memory space, making data sharing easy but\n",
    "requiring synchronization. Daemon threads are background threads that are killed\n",
    "automatically when all non-daemon threads have exited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import time\n",
    "\n",
    "\n",
    "def worker(name: str, duration: float) -> None:\n",
    "    \"\"\"Simulate a task that takes some time.\"\"\"\n",
    "    thread = threading.current_thread()\n",
    "    print(f\"[{name}] Starting on thread '{thread.name}' (daemon={thread.daemon})\")\n",
    "    time.sleep(duration)\n",
    "    print(f\"[{name}] Finished after {duration}s\")\n",
    "\n",
    "\n",
    "# Create and start threads\n",
    "threads: list[threading.Thread] = []\n",
    "for i, duration in enumerate([0.5, 0.3, 0.4], start=1):\n",
    "    t = threading.Thread(target=worker, args=(f\"Task-{i}\", duration), name=f\"Worker-{i}\")\n",
    "    threads.append(t)\n",
    "    t.start()\n",
    "\n",
    "# Wait for all threads to complete\n",
    "for t in threads:\n",
    "    t.join()\n",
    "\n",
    "print(f\"\\nAll threads completed. Active thread count: {threading.active_count()}\")\n",
    "\n",
    "\n",
    "# Daemon thread example: runs in background, killed when main thread exits\n",
    "def background_monitor(interval: float) -> None:\n",
    "    \"\"\"A daemon thread that monitors something periodically.\"\"\"\n",
    "    while True:\n",
    "        print(f\"  [Monitor] Active threads: {threading.active_count()}\")\n",
    "        time.sleep(interval)\n",
    "\n",
    "\n",
    "monitor = threading.Thread(target=background_monitor, args=(0.2,), daemon=True)\n",
    "monitor.start()\n",
    "\n",
    "print(f\"\\nMonitor thread is daemon: {monitor.daemon}\")\n",
    "print(f\"Monitor thread is alive: {monitor.is_alive()}\")\n",
    "time.sleep(0.5)  # Let the monitor run briefly\n",
    "print(\"Main thread done -- daemon will be stopped automatically\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## The Global Interpreter Lock (GIL)\n",
    "\n",
    "CPython has a Global Interpreter Lock that allows only one thread to execute\n",
    "Python bytecode at a time. This means:\n",
    "\n",
    "- **I/O-bound tasks** benefit from threading because threads release the GIL\n",
    "  while waiting for I/O (network, disk, sleep).\n",
    "- **CPU-bound tasks** do NOT benefit from threading. Multiple threads will\n",
    "  effectively run one at a time, sometimes even slower due to context switching.\n",
    "- For CPU-bound parallelism, use `multiprocessing` or `ProcessPoolExecutor`.\n",
    "\n",
    "The demonstration below shows threading helping I/O-bound work but not CPU-bound work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import time\n",
    "\n",
    "\n",
    "def io_bound_task(task_id: int) -> str:\n",
    "    \"\"\"Simulate I/O-bound work (network call, file read, etc.).\"\"\"\n",
    "    time.sleep(0.5)  # Simulates waiting for I/O\n",
    "    return f\"IO-{task_id} done\"\n",
    "\n",
    "\n",
    "def cpu_bound_task(n: int) -> int:\n",
    "    \"\"\"CPU-bound work: sum of squares.\"\"\"\n",
    "    return sum(i * i for i in range(n))\n",
    "\n",
    "\n",
    "# --- I/O-bound: sequential vs threaded ---\n",
    "NUM_TASKS = 6\n",
    "\n",
    "start = time.perf_counter()\n",
    "for i in range(NUM_TASKS):\n",
    "    io_bound_task(i)\n",
    "sequential_io = time.perf_counter() - start\n",
    "\n",
    "start = time.perf_counter()\n",
    "threads = [threading.Thread(target=io_bound_task, args=(i,)) for i in range(NUM_TASKS)]\n",
    "for t in threads:\n",
    "    t.start()\n",
    "for t in threads:\n",
    "    t.join()\n",
    "threaded_io = time.perf_counter() - start\n",
    "\n",
    "print(\"=== I/O-bound (threading HELPS) ===\")\n",
    "print(f\"Sequential: {sequential_io:.2f}s\")\n",
    "print(f\"Threaded:   {threaded_io:.2f}s\")\n",
    "print(f\"Speedup:    {sequential_io / threaded_io:.1f}x\")\n",
    "\n",
    "\n",
    "# --- CPU-bound: sequential vs threaded ---\n",
    "N = 2_000_000\n",
    "\n",
    "start = time.perf_counter()\n",
    "for _ in range(NUM_TASKS):\n",
    "    cpu_bound_task(N)\n",
    "sequential_cpu = time.perf_counter() - start\n",
    "\n",
    "start = time.perf_counter()\n",
    "threads = [threading.Thread(target=cpu_bound_task, args=(N,)) for _ in range(NUM_TASKS)]\n",
    "for t in threads:\n",
    "    t.start()\n",
    "for t in threads:\n",
    "    t.join()\n",
    "threaded_cpu = time.perf_counter() - start\n",
    "\n",
    "print(\"\\n=== CPU-bound (threading does NOT help due to GIL) ===\")\n",
    "print(f\"Sequential: {sequential_cpu:.2f}s\")\n",
    "print(f\"Threaded:   {threaded_cpu:.2f}s\")\n",
    "print(f\"Speedup:    {sequential_cpu / threaded_cpu:.1f}x  (expect ~1x or worse)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Thread Synchronization with Locks\n",
    "\n",
    "When threads share mutable state, race conditions can occur. A `Lock` ensures\n",
    "only one thread can access the critical section at a time. Always use locks\n",
    "as context managers (`with lock:`) to guarantee release even on exceptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "\n",
    "class UnsafeCounter:\n",
    "    \"\"\"Counter WITHOUT synchronization -- prone to race conditions.\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.value: int = 0\n",
    "\n",
    "    def increment(self) -> None:\n",
    "        # Read-modify-write is NOT atomic: another thread can interleave\n",
    "        current = self.value\n",
    "        self.value = current + 1\n",
    "\n",
    "\n",
    "class SafeCounter:\n",
    "    \"\"\"Counter WITH lock-based synchronization.\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.value: int = 0\n",
    "        self._lock = threading.Lock()\n",
    "\n",
    "    def increment(self) -> None:\n",
    "        with self._lock:  # Only one thread enters this block at a time\n",
    "            current = self.value\n",
    "            self.value = current + 1\n",
    "\n",
    "\n",
    "def run_increments(counter: UnsafeCounter | SafeCounter, num_threads: int, per_thread: int) -> int:\n",
    "    \"\"\"Spawn threads to increment a shared counter.\"\"\"\n",
    "    def target() -> None:\n",
    "        for _ in range(per_thread):\n",
    "            counter.increment()\n",
    "\n",
    "    threads = [threading.Thread(target=target) for _ in range(num_threads)]\n",
    "    for t in threads:\n",
    "        t.start()\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "    return counter.value\n",
    "\n",
    "\n",
    "NUM_THREADS = 10\n",
    "PER_THREAD = 10_000\n",
    "EXPECTED = NUM_THREADS * PER_THREAD\n",
    "\n",
    "# Unsafe counter may lose increments\n",
    "unsafe = UnsafeCounter()\n",
    "unsafe_result = run_increments(unsafe, NUM_THREADS, PER_THREAD)\n",
    "print(f\"Unsafe counter: {unsafe_result:,} (expected {EXPECTED:,}, lost {EXPECTED - unsafe_result:,})\")\n",
    "\n",
    "# Safe counter is always correct\n",
    "safe = SafeCounter()\n",
    "safe_result = run_increments(safe, NUM_THREADS, PER_THREAD)\n",
    "print(f\"Safe counter:   {safe_result:,} (expected {EXPECTED:,}, lost {EXPECTED - safe_result:,})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## concurrent.futures: High-Level Concurrency\n",
    "\n",
    "The `concurrent.futures` module provides a clean, unified API for both threading\n",
    "and multiprocessing through `ThreadPoolExecutor` and `ProcessPoolExecutor`.\n",
    "The `Future` object represents a pending result, and `as_completed` yields\n",
    "futures as they finish (not in submission order)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed, Future\n",
    "import time\n",
    "\n",
    "\n",
    "def fetch_url(url: str) -> dict[str, str | float]:\n",
    "    \"\"\"Simulate fetching a URL with variable latency.\"\"\"\n",
    "    latency = len(url) % 5 * 0.1 + 0.1  # Deterministic fake latency\n",
    "    time.sleep(latency)\n",
    "    return {\"url\": url, \"status\": \"200 OK\", \"latency\": round(latency, 2)}\n",
    "\n",
    "\n",
    "urls: list[str] = [\n",
    "    \"https://api.example.com/users\",\n",
    "    \"https://api.example.com/products\",\n",
    "    \"https://api.example.com/orders\",\n",
    "    \"https://api.example.com/inventory\",\n",
    "    \"https://api.example.com/analytics\",\n",
    "]\n",
    "\n",
    "# --- submit() + as_completed: process results as they arrive ---\n",
    "print(\"=== ThreadPoolExecutor with as_completed ===\")\n",
    "start = time.perf_counter()\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "    # Submit all tasks, mapping futures back to their input\n",
    "    future_to_url: dict[Future[dict[str, str | float]], str] = {\n",
    "        executor.submit(fetch_url, url): url for url in urls\n",
    "    }\n",
    "\n",
    "    for future in as_completed(future_to_url):\n",
    "        url = future_to_url[future]\n",
    "        try:\n",
    "            result = future.result()  # Raises if the callable raised\n",
    "            print(f\"  {result['url']} -> {result['status']} ({result['latency']}s)\")\n",
    "        except Exception as exc:\n",
    "            print(f\"  {url} generated an exception: {exc}\")\n",
    "\n",
    "print(f\"  Total time: {time.perf_counter() - start:.2f}s\\n\")\n",
    "\n",
    "\n",
    "# --- executor.map(): simpler API, results in submission order ---\n",
    "print(\"=== ThreadPoolExecutor with map (ordered results) ===\")\n",
    "start = time.perf_counter()\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "    results = executor.map(fetch_url, urls)\n",
    "    for result in results:  # Results arrive in submission order\n",
    "        print(f\"  {result['url']} -> {result['status']}\")\n",
    "\n",
    "print(f\"  Total time: {time.perf_counter() - start:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## ProcessPoolExecutor for CPU-Bound Work\n",
    "\n",
    "`ProcessPoolExecutor` spawns separate processes, each with its own Python\n",
    "interpreter and GIL. This achieves true parallelism for CPU-bound tasks.\n",
    "The API is identical to `ThreadPoolExecutor`, making it easy to swap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def is_prime(n: int) -> bool:\n",
    "    \"\"\"CPU-intensive primality test.\"\"\"\n",
    "    if n < 2:\n",
    "        return False\n",
    "    if n < 4:\n",
    "        return True\n",
    "    if n % 2 == 0 or n % 3 == 0:\n",
    "        return False\n",
    "    for i in range(5, int(math.isqrt(n)) + 1, 6):\n",
    "        if n % i == 0 or n % (i + 2) == 0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "# Large numbers to test primality\n",
    "candidates: list[int] = [\n",
    "    15485863, 15485867, 32452843, 32452847,\n",
    "    49979687, 49979693, 67867967, 67867979,\n",
    "]\n",
    "\n",
    "# Sequential\n",
    "start = time.perf_counter()\n",
    "sequential_results = [is_prime(n) for n in candidates]\n",
    "seq_time = time.perf_counter() - start\n",
    "\n",
    "# Threaded (limited by GIL for CPU work)\n",
    "start = time.perf_counter()\n",
    "with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "    thread_results = list(executor.map(is_prime, candidates))\n",
    "thread_time = time.perf_counter() - start\n",
    "\n",
    "# Multiprocess (true parallelism, bypasses GIL)\n",
    "start = time.perf_counter()\n",
    "with ProcessPoolExecutor(max_workers=4) as executor:\n",
    "    process_results = list(executor.map(is_prime, candidates))\n",
    "process_time = time.perf_counter() - start\n",
    "\n",
    "print(\"=== CPU-bound: Primality Testing ===\")\n",
    "print(f\"Sequential:        {seq_time:.4f}s\")\n",
    "print(f\"ThreadPool (4w):   {thread_time:.4f}s  (speedup: {seq_time/thread_time:.2f}x)\")\n",
    "print(f\"ProcessPool (4w):  {process_time:.4f}s  (speedup: {seq_time/process_time:.2f}x)\")\n",
    "\n",
    "print(\"\\nResults:\")\n",
    "for n, prime in zip(candidates, process_results):\n",
    "    print(f\"  {n:>10,} -> {'prime' if prime else 'composite'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Async/Await with asyncio\n",
    "\n",
    "The `asyncio` module provides cooperative multitasking using coroutines.\n",
    "Unlike threads, coroutines run on a single thread and yield control\n",
    "explicitly with `await`. This avoids race conditions and is ideal\n",
    "for high-concurrency I/O (thousands of simultaneous connections).\n",
    "\n",
    "Key concepts:\n",
    "- **coroutine**: Defined with `async def`, must be awaited\n",
    "- **await**: Suspends the coroutine until the awaited operation completes\n",
    "- **gather**: Runs multiple coroutines concurrently\n",
    "- **event loop**: Schedules and runs coroutines (managed automatically)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import time\n",
    "\n",
    "\n",
    "async def fetch_data(source: str, delay: float) -> dict[str, str | float]:\n",
    "    \"\"\"Simulate an async I/O operation (e.g., HTTP request, DB query).\"\"\"\n",
    "    print(f\"  [{source}] Starting fetch...\")\n",
    "    await asyncio.sleep(delay)  # Non-blocking sleep; yields control to event loop\n",
    "    print(f\"  [{source}] Completed after {delay}s\")\n",
    "    return {\"source\": source, \"data\": f\"result from {source}\", \"latency\": delay}\n",
    "\n",
    "\n",
    "async def main_sequential() -> list[dict[str, str | float]]:\n",
    "    \"\"\"Run coroutines one at a time (no concurrency).\"\"\"\n",
    "    results = []\n",
    "    for source, delay in [(\"DB\", 0.5), (\"API\", 0.3), (\"Cache\", 0.1)]:\n",
    "        result = await fetch_data(source, delay)  # Waits for each one\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "async def main_concurrent() -> list[dict[str, str | float]]:\n",
    "    \"\"\"Run coroutines concurrently with gather.\"\"\"\n",
    "    results = await asyncio.gather(\n",
    "        fetch_data(\"DB\", 0.5),\n",
    "        fetch_data(\"API\", 0.3),\n",
    "        fetch_data(\"Cache\", 0.1),\n",
    "    )\n",
    "    return list(results)\n",
    "\n",
    "\n",
    "# Sequential execution\n",
    "print(\"=== Sequential (await one at a time) ===\")\n",
    "start = time.perf_counter()\n",
    "seq_results = await main_sequential()\n",
    "seq_time = time.perf_counter() - start\n",
    "print(f\"  Total: {seq_time:.2f}s\\n\")\n",
    "\n",
    "# Concurrent execution\n",
    "print(\"=== Concurrent (asyncio.gather) ===\")\n",
    "start = time.perf_counter()\n",
    "con_results = await main_concurrent()\n",
    "con_time = time.perf_counter() - start\n",
    "print(f\"  Total: {con_time:.2f}s\")\n",
    "print(f\"  Speedup: {seq_time / con_time:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Async Patterns: Tasks and Timeouts\n",
    "\n",
    "`asyncio.create_task()` schedules a coroutine to run in the background.\n",
    "`asyncio.wait_for()` adds a timeout to any awaitable. These patterns\n",
    "give fine-grained control over concurrent async operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "\n",
    "async def slow_operation(name: str, delay: float) -> str:\n",
    "    \"\"\"A coroutine that might take too long.\"\"\"\n",
    "    await asyncio.sleep(delay)\n",
    "    return f\"{name} completed\"\n",
    "\n",
    "\n",
    "async def demonstrate_tasks() -> None:\n",
    "    \"\"\"Show create_task for background scheduling.\"\"\"\n",
    "    print(\"=== asyncio.create_task ===\")\n",
    "\n",
    "    # Tasks start running immediately when created\n",
    "    task_a = asyncio.create_task(slow_operation(\"Task-A\", 0.3))\n",
    "    task_b = asyncio.create_task(slow_operation(\"Task-B\", 0.1))\n",
    "\n",
    "    print(f\"  Task-A done? {task_a.done()}\")\n",
    "    print(f\"  Task-B done? {task_b.done()}\")\n",
    "\n",
    "    # Await results\n",
    "    result_a = await task_a\n",
    "    result_b = await task_b\n",
    "    print(f\"  Results: {result_a}, {result_b}\")\n",
    "\n",
    "\n",
    "async def demonstrate_timeout() -> None:\n",
    "    \"\"\"Show wait_for with timeout handling.\"\"\"\n",
    "    print(\"\\n=== asyncio.wait_for with timeout ===\")\n",
    "\n",
    "    # This completes in time\n",
    "    try:\n",
    "        result = await asyncio.wait_for(slow_operation(\"Fast\", 0.1), timeout=1.0)\n",
    "        print(f\"  Success: {result}\")\n",
    "    except asyncio.TimeoutError:\n",
    "        print(\"  Timed out!\")\n",
    "\n",
    "    # This exceeds the timeout\n",
    "    try:\n",
    "        result = await asyncio.wait_for(slow_operation(\"Slow\", 5.0), timeout=0.2)\n",
    "        print(f\"  Success: {result}\")\n",
    "    except asyncio.TimeoutError:\n",
    "        print(\"  Timed out! (as expected for the slow operation)\")\n",
    "\n",
    "\n",
    "async def demonstrate_exception_handling() -> None:\n",
    "    \"\"\"Show exception handling with gather.\"\"\"\n",
    "    print(\"\\n=== Exception handling in gather ===\")\n",
    "\n",
    "    async def might_fail(name: str, should_fail: bool) -> str:\n",
    "        await asyncio.sleep(0.1)\n",
    "        if should_fail:\n",
    "            raise ValueError(f\"{name} failed!\")\n",
    "        return f\"{name} succeeded\"\n",
    "\n",
    "    # return_exceptions=True prevents one failure from cancelling others\n",
    "    results = await asyncio.gather(\n",
    "        might_fail(\"A\", False),\n",
    "        might_fail(\"B\", True),\n",
    "        might_fail(\"C\", False),\n",
    "        return_exceptions=True,\n",
    "    )\n",
    "\n",
    "    for i, result in enumerate(results):\n",
    "        if isinstance(result, Exception):\n",
    "            print(f\"  Task {i}: FAILED - {result}\")\n",
    "        else:\n",
    "            print(f\"  Task {i}: {result}\")\n",
    "\n",
    "\n",
    "await demonstrate_tasks()\n",
    "await demonstrate_timeout()\n",
    "await demonstrate_exception_handling()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## When to Use Threads vs Processes vs Asyncio\n",
    "\n",
    "| Criterion | `threading` | `multiprocessing` | `asyncio` |\n",
    "|---|---|---|---|\n",
    "| **Best for** | I/O-bound (moderate concurrency) | CPU-bound (true parallelism) | I/O-bound (high concurrency) |\n",
    "| **GIL impact** | Limited by GIL for CPU work | Bypasses GIL (separate processes) | Single-threaded, no GIL issue |\n",
    "| **Memory** | Shared (needs locks) | Separate (needs IPC) | Shared (no locks needed) |\n",
    "| **Overhead** | Low (lightweight) | High (process creation) | Very low (coroutines) |\n",
    "| **Scalability** | ~100s of threads | ~10s of processes | ~10,000s of coroutines |\n",
    "| **Debugging** | Hard (race conditions) | Moderate (isolation helps) | Easier (single thread) |\n",
    "| **Use case** | File I/O, simple web scraping | Data processing, math | Web servers, API clients |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary: choosing the right concurrency model\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum, auto\n",
    "\n",
    "\n",
    "class WorkloadType(Enum):\n",
    "    IO_BOUND = auto()\n",
    "    CPU_BOUND = auto()\n",
    "\n",
    "\n",
    "class ConcurrencyLevel(Enum):\n",
    "    LOW = auto()       # < 100 concurrent tasks\n",
    "    HIGH = auto()      # 100s to 1000s of concurrent tasks\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ConcurrencyAdvice:\n",
    "    model: str\n",
    "    module: str\n",
    "    reasoning: str\n",
    "\n",
    "\n",
    "def recommend_concurrency(\n",
    "    workload: WorkloadType,\n",
    "    concurrency: ConcurrencyLevel,\n",
    ") -> ConcurrencyAdvice:\n",
    "    \"\"\"Recommend the appropriate concurrency model.\"\"\"\n",
    "    if workload == WorkloadType.CPU_BOUND:\n",
    "        return ConcurrencyAdvice(\n",
    "            model=\"Multiprocessing\",\n",
    "            module=\"concurrent.futures.ProcessPoolExecutor\",\n",
    "            reasoning=\"CPU-bound work needs separate processes to bypass the GIL\",\n",
    "        )\n",
    "    # I/O-bound\n",
    "    if concurrency == ConcurrencyLevel.HIGH:\n",
    "        return ConcurrencyAdvice(\n",
    "            model=\"Asyncio\",\n",
    "            module=\"asyncio\",\n",
    "            reasoning=\"High-concurrency I/O benefits from lightweight coroutines\",\n",
    "        )\n",
    "    return ConcurrencyAdvice(\n",
    "        model=\"Threading\",\n",
    "        module=\"concurrent.futures.ThreadPoolExecutor\",\n",
    "        reasoning=\"Moderate I/O concurrency is well-served by threads\",\n",
    "    )\n",
    "\n",
    "\n",
    "# Example scenarios\n",
    "scenarios: list[tuple[str, WorkloadType, ConcurrencyLevel]] = [\n",
    "    (\"Image processing pipeline\", WorkloadType.CPU_BOUND, ConcurrencyLevel.LOW),\n",
    "    (\"Web scraper (50 URLs)\", WorkloadType.IO_BOUND, ConcurrencyLevel.LOW),\n",
    "    (\"Async web server\", WorkloadType.IO_BOUND, ConcurrencyLevel.HIGH),\n",
    "    (\"ML model training\", WorkloadType.CPU_BOUND, ConcurrencyLevel.HIGH),\n",
    "]\n",
    "\n",
    "for name, workload, level in scenarios:\n",
    "    advice = recommend_concurrency(workload, level)\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Model:  {advice.model} ({advice.module})\")\n",
    "    print(f\"  Why:    {advice.reasoning}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}