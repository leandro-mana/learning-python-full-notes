{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 11: Unicode Deep Dive\n",
    "\n",
    "Unicode is the universal standard for representing text from every writing system. Python's `str` type is built on Unicode, and the `unicodedata` module provides tools for inspecting and manipulating Unicode characters. This notebook covers code points, character properties, normalization forms, and comparison gotchas.\n",
    "\n",
    "## Key Concepts\n",
    "- **Code point**: A unique integer assigned to each character (e.g., U+0041 = 'A')\n",
    "- **Character name**: A descriptive name for each code point (e.g., LATIN CAPITAL LETTER A)\n",
    "- **Category**: Two-letter classification (e.g., Lu = Letter, uppercase)\n",
    "- **Normalization**: Canonical equivalence between different representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unicode Code Points and Character Names\n",
    "\n",
    "Every Unicode character has a unique code point (an integer) and a name. Python uses `ord()` and `chr()` to convert between characters and code points, and `unicodedata.name()` to look up names."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import unicodedata\n",
    "\n",
    "# Code points and character names\n",
    "characters: list[str] = [\"A\", \"√©\", \"Œ±\", \"‰∏≠\", \"‚Ç¨\", \"üêç\"]\n",
    "\n",
    "print(f\"{'Char':<6} {'Code Point':<12} {'Name':<40} {'Category'}\")\n",
    "print(\"-\" * 75)\n",
    "for char in characters:\n",
    "    cp = ord(char)\n",
    "    name = unicodedata.name(char, \"<unknown>\")\n",
    "    cat = unicodedata.category(char)\n",
    "    print(f\"{char:<6} U+{cp:04X}{'':>5} {name:<40} {cat}\")\n",
    "\n",
    "# Reverse lookup: name -> character\n",
    "euro = unicodedata.lookup(\"EURO SIGN\")\n",
    "print(f\"\\nunicodedata.lookup('EURO SIGN') = {euro!r}\")\n",
    "\n",
    "# Using \\N{} escape in string literals\n",
    "snowman = \"\\N{SNOWMAN}\"\n",
    "print(f\"\\\\N{{SNOWMAN}} = {snowman}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The unicodedata Module\n",
    "\n",
    "The `unicodedata` module provides access to the Unicode Character Database. Key functions include `name()`, `category()`, `normalize()`, `numeric()`, and `bidirectional()`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import unicodedata\n",
    "\n",
    "# Exploring character properties\n",
    "char: str = \"√©\"\n",
    "print(f\"Character: {char!r}\")\n",
    "print(f\"Name: {unicodedata.name(char)}\")\n",
    "print(f\"Category: {unicodedata.category(char)}\")\n",
    "print(f\"Decimal: {unicodedata.decimal(char, None)}\")\n",
    "print(f\"Numeric: {unicodedata.numeric(char, None)}\")\n",
    "print(f\"Bidirectional: {unicodedata.bidirectional(char)}\")\n",
    "\n",
    "# Numeric values for various scripts\n",
    "numeric_chars: list[str] = [\"5\", \"\\u0665\", \"\\u0e55\", \"\\u4e94\", \"\\u2165\"]\n",
    "print(f\"\\n{'Char':<6} {'Name':<35} {'Numeric Value'}\")\n",
    "print(\"-\" * 60)\n",
    "for ch in numeric_chars:\n",
    "    name = unicodedata.name(ch, \"<unknown>\")\n",
    "    num = unicodedata.numeric(ch, None)\n",
    "    print(f\"{ch:<6} {name:<35} {num}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unicode Categories\n",
    "\n",
    "Each Unicode character belongs to a **general category** identified by a two-letter code. The first letter indicates the major class, the second letter the subclass.\n",
    "\n",
    "| Code | Meaning | Example |\n",
    "|------|---------|--------|\n",
    "| Lu | Letter, uppercase | A, B, Z |\n",
    "| Ll | Letter, lowercase | a, b, z |\n",
    "| Nd | Number, decimal digit | 0-9, Arabic-Indic digits |\n",
    "| Zs | Separator, space | space, no-break space |\n",
    "| Mn | Mark, nonspacing | combining accents |\n",
    "| Sc | Symbol, currency | $, EUR, GBP |\n",
    "| So | Symbol, other | emojis, misc symbols |"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import unicodedata\n",
    "\n",
    "# Demonstrating Unicode categories\n",
    "test_chars: list[tuple[str, str]] = [\n",
    "    (\"A\", \"uppercase letter\"),\n",
    "    (\"a\", \"lowercase letter\"),\n",
    "    (\"3\", \"decimal digit\"),\n",
    "    (\" \", \"space\"),\n",
    "    (\"\\u0301\", \"combining acute accent\"),\n",
    "    (\"$\", \"dollar sign\"),\n",
    "    (\"‚Ç¨\", \"euro sign\"),\n",
    "    (\"\\u00A0\", \"no-break space\"),\n",
    "]\n",
    "\n",
    "print(f\"{'Char':<8} {'Repr':<12} {'Category':<10} {'Description'}\")\n",
    "print(\"-\" * 55)\n",
    "for char, desc in test_chars:\n",
    "    cat = unicodedata.category(char)\n",
    "    print(f\"{char:<8} {char!r:<12} {cat:<10} {desc}\")\n",
    "\n",
    "# Verify specific categories from the test file\n",
    "assert unicodedata.category(\"A\") == \"Lu\"  # Letter, uppercase\n",
    "assert unicodedata.category(\"3\") == \"Nd\"  # Number, decimal digit\n",
    "assert unicodedata.category(\" \") == \"Zs\"  # Separator, space\n",
    "print(\"\\nAll category assertions passed.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NFC vs NFD Normalization\n",
    "\n",
    "Some characters can be represented in multiple ways. For example, 'e' (U+00E9) can be:\n",
    "- **NFC** (Composed): A single code point `\\u00e9`\n",
    "- **NFD** (Decomposed): Base letter `e` (U+0065) + combining acute accent (U+0301)\n",
    "\n",
    "These look identical on screen but are different sequences of code points, which affects string comparison."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import unicodedata\n",
    "\n",
    "# NFC (Composed) vs NFD (Decomposed)\n",
    "nfc: str = \"\\u00e9\"          # √© as single codepoint (LATIN SMALL LETTER E WITH ACUTE)\n",
    "nfd: str = \"\\u0065\\u0301\"   # e + combining acute accent\n",
    "\n",
    "print(f\"NFC: {nfc!r}  (len={len(nfc)})\")\n",
    "print(f\"NFD: {nfd!r}  (len={len(nfd)})\")\n",
    "print(f\"Look the same: '{nfc}' vs '{nfd}'\")\n",
    "print(f\"But are NOT equal: {nfc == nfd}\")\n",
    "\n",
    "# Normalize to compare\n",
    "nfc_normalized = unicodedata.normalize(\"NFC\", nfd)\n",
    "nfd_normalized = unicodedata.normalize(\"NFD\", nfc)\n",
    "\n",
    "print(f\"\\nNormalize NFD -> NFC: {nfc_normalized!r} (len={len(nfc_normalized)})\")\n",
    "print(f\"Normalize NFC -> NFD: {nfd_normalized!r} (len={len(nfd_normalized)})\")\n",
    "print(f\"After NFC normalization, equal: {nfc == nfc_normalized}\")\n",
    "print(f\"After NFD normalization, equal: {nfd == nfd_normalized}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import unicodedata\n",
    "\n",
    "# NFKC and NFKD: compatibility normalization\n",
    "# These also replace compatibility characters with their canonical forms\n",
    "\n",
    "# Example: the 'fi' ligature and the superscript '2'\n",
    "ligature: str = \"\\ufb01\"  # fi ligature\n",
    "superscript: str = \"\\u00b2\"  # superscript 2\n",
    "half: str = \"\\u00bd\"  # vulgar fraction one half\n",
    "\n",
    "print(f\"{'Original':<12} {'NFKC':<12} {'NFKD':<12} {'Description'}\")\n",
    "print(\"-\" * 55)\n",
    "for char, desc in [(ligature, \"fi ligature\"), (superscript, \"superscript 2\"), (half, \"fraction 1/2\")]:\n",
    "    nfkc = unicodedata.normalize(\"NFKC\", char)\n",
    "    nfkd = unicodedata.normalize(\"NFKD\", char)\n",
    "    print(f\"{char!r:<12} {nfkc!r:<12} {nfkd!r:<12} {desc}\")\n",
    "\n",
    "# Practical use: search normalization\n",
    "def normalize_for_search(text: str) -> str:\n",
    "    \"\"\"Normalize text for case-insensitive, accent-insensitive search.\"\"\"\n",
    "    # NFKD decomposes, then we can strip combining marks\n",
    "    decomposed = unicodedata.normalize(\"NFKD\", text)\n",
    "    # Remove combining characters (category 'M')\n",
    "    stripped = \"\".join(c for c in decomposed if not unicodedata.category(c).startswith(\"M\"))\n",
    "    return stripped.casefold()\n",
    "\n",
    "print(f\"\\nSearch normalization examples:\")\n",
    "for word in [\"caf√©\", \"CAF√â\", \"r√©sum√©\", \"na√Øve\"]:\n",
    "    print(f\"  {word!r:>12} -> {normalize_for_search(word)!r}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Combining Characters\n",
    "\n",
    "Combining characters (category 'Mn') are diacritical marks that attach to the preceding base character. They are zero-width and modify how the base character is displayed."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import unicodedata\n",
    "\n",
    "# Combining characters example\n",
    "base: str = \"e\"\n",
    "acute: str = \"\\u0301\"      # COMBINING ACUTE ACCENT\n",
    "cedilla: str = \"\\u0327\"    # COMBINING CEDILLA\n",
    "tilde: str = \"\\u0303\"     # COMBINING TILDE\n",
    "\n",
    "combined_e_acute = base + acute\n",
    "combined_e_cedilla = base + cedilla\n",
    "combined_n_tilde = \"n\" + tilde\n",
    "\n",
    "print(f\"e + combining acute:   {combined_e_acute!r}  displays as: {combined_e_acute}\")\n",
    "print(f\"e + combining cedilla: {combined_e_cedilla!r}  displays as: {combined_e_cedilla}\")\n",
    "print(f\"n + combining tilde:   {combined_n_tilde!r}  displays as: {combined_n_tilde}\")\n",
    "\n",
    "# Stacking multiple combining characters\n",
    "stacked = \"a\" + \"\\u0300\" + \"\\u0301\" + \"\\u0302\"  # grave + acute + circumflex\n",
    "print(f\"\\nStacked accents: {stacked!r}  displays as: {stacked}\")\n",
    "print(f\"Length: {len(stacked)} code points (but visually one character)\")\n",
    "\n",
    "# Detecting combining characters\n",
    "text = \"caf√©\"  # NFD form might have combining characters\n",
    "nfd_text = unicodedata.normalize(\"NFD\", text)\n",
    "print(f\"\\n{text!r} in NFD form: {nfd_text!r}\")\n",
    "for i, ch in enumerate(nfd_text):\n",
    "    cat = unicodedata.category(ch)\n",
    "    name = unicodedata.name(ch)\n",
    "    is_combining = cat.startswith(\"M\")\n",
    "    print(f\"  [{i}] U+{ord(ch):04X} {cat} {name}{' (combining)' if is_combining else ''}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String Comparison Gotchas with Unicode\n",
    "\n",
    "Because the same visual character can have multiple representations, direct string comparison (`==`) can produce surprising results. Always normalize before comparing."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import unicodedata\n",
    "\n",
    "# Gotcha 1: Same-looking strings that are not equal\n",
    "s1: str = \"caf\\u00e9\"         # '√©' as single code point\n",
    "s2: str = \"cafe\\u0301\"        # 'e' + combining acute\n",
    "\n",
    "print(f\"s1: {s1!r} -> {s1}\")\n",
    "print(f\"s2: {s2!r} -> {s2}\")\n",
    "print(f\"s1 == s2: {s1 == s2}  (surprise!)\")\n",
    "print(f\"len(s1)={len(s1)}, len(s2)={len(s2)}\")\n",
    "\n",
    "# Fix: normalize before comparing\n",
    "s1_nfc = unicodedata.normalize(\"NFC\", s1)\n",
    "s2_nfc = unicodedata.normalize(\"NFC\", s2)\n",
    "print(f\"\\nAfter NFC normalization:\")\n",
    "print(f\"s1_nfc == s2_nfc: {s1_nfc == s2_nfc}\")\n",
    "\n",
    "# Gotcha 2: Different characters that look similar\n",
    "latin_a: str = \"A\"         # U+0041 LATIN CAPITAL LETTER A\n",
    "greek_a: str = \"\\u0391\"   # U+0391 GREEK CAPITAL LETTER ALPHA\n",
    "cyrillic_a: str = \"\\u0410\"  # U+0410 CYRILLIC CAPITAL LETTER A\n",
    "\n",
    "print(f\"\\nHomoglyphs (look-alikes):\")\n",
    "print(f\"  Latin A:    {latin_a!r} (U+{ord(latin_a):04X})\")\n",
    "print(f\"  Greek A:    {greek_a!r} (U+{ord(greek_a):04X})\")\n",
    "print(f\"  Cyrillic A: {cyrillic_a!r} (U+{ord(cyrillic_a):04X})\")\n",
    "print(f\"  All look like 'A' but: {latin_a == greek_a} {latin_a == cyrillic_a}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## casefold() vs lower() for Case-Insensitive Comparison\n",
    "\n",
    "`str.casefold()` is more aggressive than `str.lower()` and is the correct choice for case-insensitive comparisons, especially with non-ASCII text. It handles special cases like the German eszett (sharp s)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# casefold() vs lower()\n",
    "# For ASCII text, they behave the same\n",
    "ascii_text: str = \"HELLO WORLD\"\n",
    "print(f\"ASCII lower():    {ascii_text.lower()!r}\")\n",
    "print(f\"ASCII casefold(): {ascii_text.casefold()!r}\")\n",
    "\n",
    "# For the German eszett, they differ\n",
    "eszett: str = \"Stra√üe\"  # German word with sharp s\n",
    "print(f\"\\nOriginal:  {eszett!r}\")\n",
    "print(f\"lower():   {eszett.lower()!r}\")\n",
    "print(f\"casefold(): {eszett.casefold()!r}\")\n",
    "print(f\"Note: casefold() converts '√ü' to 'ss'\")\n",
    "\n",
    "# Practical comparison function\n",
    "def case_insensitive_equal(a: str, b: str) -> bool:\n",
    "    \"\"\"Compare strings case-insensitively using casefold.\"\"\"\n",
    "    return a.casefold() == b.casefold()\n",
    "\n",
    "# Test cases\n",
    "pairs: list[tuple[str, str]] = [\n",
    "    (\"hello\", \"HELLO\"),\n",
    "    (\"Stra√üe\", \"STRASSE\"),\n",
    "    (\"caf√©\", \"CAF√â\"),\n",
    "    (\"Œ£ŒØœÉœÖœÜŒøœÇ\", \"Œ£ŒäŒ£Œ•Œ¶ŒüŒ£\"),\n",
    "]\n",
    "print(f\"\\n{'String A':<15} {'String B':<15} {'lower==':<10} {'casefold=='}\")\n",
    "print(\"-\" * 55)\n",
    "for a, b in pairs:\n",
    "    lower_eq = a.lower() == b.lower()\n",
    "    fold_eq = a.casefold() == b.casefold()\n",
    "    print(f\"{a:<15} {b:<15} {str(lower_eq):<10} {fold_eq}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import unicodedata\n",
    "\n",
    "# Putting it all together: a robust string comparison utility\n",
    "def unicode_equal(\n",
    "    a: str,\n",
    "    b: str,\n",
    "    *,\n",
    "    case_sensitive: bool = True,\n",
    "    normalization: str = \"NFC\",\n",
    ") -> bool:\n",
    "    \"\"\"Compare two Unicode strings with normalization and optional case folding.\"\"\"\n",
    "    a_norm = unicodedata.normalize(normalization, a)\n",
    "    b_norm = unicodedata.normalize(normalization, b)\n",
    "    if not case_sensitive:\n",
    "        a_norm = a_norm.casefold()\n",
    "        b_norm = b_norm.casefold()\n",
    "    return a_norm == b_norm\n",
    "\n",
    "# Test with various tricky cases\n",
    "print(\"Robust Unicode comparison:\")\n",
    "print(f\"  NFC vs NFD 'caf√©':  {unicode_equal('caf\\u00e9', 'cafe\\u0301')}\")\n",
    "print(f\"  Case-insensitive:   {unicode_equal('Caf√©', 'CAF√â', case_sensitive=False)}\")\n",
    "print(f\"  Stra√üe vs STRASSE:  {unicode_equal('Stra√üe', 'STRASSE', case_sensitive=False)}\")\n",
    "print(f\"  Exact match:        {unicode_equal('hello', 'Hello', case_sensitive=True)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- Every Unicode character has a **code point** (`ord()`) and a **name** (`unicodedata.name()`)\n",
    "- Characters belong to **categories** (Lu, Ll, Nd, Zs, Mn, etc.) accessible via `unicodedata.category()`\n",
    "- **NFC** (composed) and **NFD** (decomposed) are different representations of the same character\n",
    "- **NFKC/NFKD** additionally replace compatibility characters (ligatures, superscripts)\n",
    "- Always **normalize** strings before comparison to avoid false mismatches\n",
    "- Use `casefold()` instead of `lower()` for case-insensitive comparison -- it handles special cases like German eszett\n",
    "- **Combining characters** (category 'Mn') attach to preceding base characters and affect string length\n",
    "- **Homoglyphs** (look-alike characters from different scripts) are not equal despite appearing identical"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}