{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Professional Testing with pytest\n",
    "\n",
    "**Chapter 10 - Learning Python, 5th Edition**\n",
    "\n",
    "Testing is fundamental to professional Python development. `pytest` is the\n",
    "de facto standard testing framework, offering powerful features like fixtures,\n",
    "parametrization, and rich assertion introspection. This notebook covers test\n",
    "structure, assertion patterns, fixtures, mocking, and best practices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Test Structure and Naming Conventions\n",
    "\n",
    "pytest discovers tests automatically based on naming conventions:\n",
    "- Test files: `test_*.py` or `*_test.py`\n",
    "- Test functions: `test_*`\n",
    "- Test classes: `Test*` (no `__init__` method)\n",
    "\n",
    "The standard pattern is **Arrange-Act-Assert (AAA)**:\n",
    "1. **Arrange**: Set up preconditions and inputs\n",
    "2. **Act**: Execute the code under test\n",
    "3. **Assert**: Verify the expected outcome\n",
    "\n",
    "Below we define small modules inline and write tests against them. In a real\n",
    "project, code and tests live in separate files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code under test: a simple calculator module\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "\n",
    "class CalculatorError(Exception):\n",
    "    \"\"\"Base exception for calculator operations.\"\"\"\n",
    "\n",
    "\n",
    "class DivisionByZeroError(CalculatorError):\n",
    "    \"\"\"Raised when dividing by zero.\"\"\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Calculator:\n",
    "    \"\"\"A calculator with history tracking.\"\"\"\n",
    "\n",
    "    history: list[str] = field(default_factory=list)\n",
    "    precision: int = 2\n",
    "\n",
    "    def add(self, a: float, b: float) -> float:\n",
    "        result = round(a + b, self.precision)\n",
    "        self.history.append(f\"{a} + {b} = {result}\")\n",
    "        return result\n",
    "\n",
    "    def subtract(self, a: float, b: float) -> float:\n",
    "        result = round(a - b, self.precision)\n",
    "        self.history.append(f\"{a} - {b} = {result}\")\n",
    "        return result\n",
    "\n",
    "    def multiply(self, a: float, b: float) -> float:\n",
    "        result = round(a * b, self.precision)\n",
    "        self.history.append(f\"{a} * {b} = {result}\")\n",
    "        return result\n",
    "\n",
    "    def divide(self, a: float, b: float) -> float:\n",
    "        if b == 0:\n",
    "            raise DivisionByZeroError(f\"Cannot divide {a} by zero\")\n",
    "        result = round(a / b, self.precision)\n",
    "        self.history.append(f\"{a} / {b} = {result}\")\n",
    "        return result\n",
    "\n",
    "    def clear_history(self) -> None:\n",
    "        self.history.clear()\n",
    "\n",
    "\n",
    "# Quick smoke test\n",
    "calc = Calculator()\n",
    "print(f\"2 + 3 = {calc.add(2, 3)}\")\n",
    "print(f\"10 / 3 = {calc.divide(10, 3)}\")\n",
    "print(f\"History: {calc.history}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Assertions and Useful Assertion Patterns\n",
    "\n",
    "pytest uses plain `assert` statements with rich introspection -- when an\n",
    "assertion fails, pytest shows the actual values of both sides. No need for\n",
    "`assertEqual`, `assertTrue`, etc. Here we demonstrate tests using the\n",
    "AAA pattern with various assertion techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test functions following AAA pattern (Arrange-Act-Assert)\n",
    "# In a real project these would be in test_calculator.py\n",
    "\n",
    "def test_add_integers() -> None:\n",
    "    \"\"\"Test addition of two integers.\"\"\"\n",
    "    # Arrange\n",
    "    calc = Calculator()\n",
    "\n",
    "    # Act\n",
    "    result = calc.add(2, 3)\n",
    "\n",
    "    # Assert\n",
    "    assert result == 5\n",
    "\n",
    "\n",
    "def test_add_floats_precision() -> None:\n",
    "    \"\"\"Test that floating point results are rounded to specified precision.\"\"\"\n",
    "    calc = Calculator(precision=3)\n",
    "    result = calc.add(1.1111, 2.2222)\n",
    "    assert result == 3.333  # Rounded to 3 decimal places\n",
    "\n",
    "\n",
    "def test_divide_returns_float() -> None:\n",
    "    \"\"\"Test that division returns a float.\"\"\"\n",
    "    calc = Calculator()\n",
    "    result = calc.divide(10, 3)\n",
    "    assert isinstance(result, float)\n",
    "    assert result == 3.33  # default precision=2\n",
    "\n",
    "\n",
    "def test_history_tracking() -> None:\n",
    "    \"\"\"Test that operations are recorded in history.\"\"\"\n",
    "    calc = Calculator()\n",
    "    calc.add(1, 2)\n",
    "    calc.multiply(3, 4)\n",
    "\n",
    "    assert len(calc.history) == 2\n",
    "    assert \"1 + 2 = 3\" in calc.history\n",
    "    assert \"3 * 4 = 12\" in calc.history\n",
    "\n",
    "\n",
    "def test_clear_history() -> None:\n",
    "    \"\"\"Test that clear_history empties the history list.\"\"\"\n",
    "    calc = Calculator()\n",
    "    calc.add(1, 2)\n",
    "    calc.clear_history()\n",
    "    assert calc.history == []\n",
    "\n",
    "\n",
    "# Approximate equality for floating point (pytest.approx equivalent)\n",
    "def test_float_comparison() -> None:\n",
    "    \"\"\"Demonstrate floating-point comparison challenges.\"\"\"\n",
    "    # This is dangerous: 0.1 + 0.2 != 0.3 due to float representation\n",
    "    assert 0.1 + 0.2 != 0.3  # Surprising but true!\n",
    "\n",
    "    # In pytest, use pytest.approx:\n",
    "    # assert 0.1 + 0.2 == pytest.approx(0.3)\n",
    "    # Here we show the manual approach:\n",
    "    assert abs((0.1 + 0.2) - 0.3) < 1e-9\n",
    "\n",
    "\n",
    "# Run all tests and report\n",
    "tests = [\n",
    "    test_add_integers,\n",
    "    test_add_floats_precision,\n",
    "    test_divide_returns_float,\n",
    "    test_history_tracking,\n",
    "    test_clear_history,\n",
    "    test_float_comparison,\n",
    "]\n",
    "\n",
    "for test_func in tests:\n",
    "    try:\n",
    "        test_func()\n",
    "        print(f\"  PASSED: {test_func.__name__}\")\n",
    "    except AssertionError as e:\n",
    "        print(f\"  FAILED: {test_func.__name__} - {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## pytest Fixtures\n",
    "\n",
    "Fixtures provide reusable setup/teardown logic. They replace the traditional\n",
    "`setUp`/`tearDown` from unittest with a more flexible, composable model.\n",
    "\n",
    "Key fixture concepts:\n",
    "- **Scope**: `function` (default), `class`, `module`, `session`\n",
    "- **yield fixtures**: Code after `yield` runs as teardown\n",
    "- **conftest.py**: Shared fixtures available to all tests in the directory\n",
    "- **Fixture composition**: Fixtures can depend on other fixtures\n",
    "\n",
    "Below we simulate fixture behavior since we cannot run pytest directly in a notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulating pytest fixtures as they would appear in test files\n",
    "from typing import Generator\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "\n",
    "# --- What fixtures look like in real pytest code ---\n",
    "\n",
    "# @pytest.fixture\n",
    "def calculator_fixture() -> Calculator:\n",
    "    \"\"\"Provide a fresh Calculator instance for each test.\"\"\"\n",
    "    return Calculator(precision=2)\n",
    "\n",
    "\n",
    "# @pytest.fixture\n",
    "def preloaded_calculator_fixture() -> Calculator:\n",
    "    \"\"\"Provide a Calculator with pre-existing history.\"\"\"\n",
    "    calc = Calculator()\n",
    "    calc.add(10, 20)\n",
    "    calc.multiply(5, 5)\n",
    "    return calc\n",
    "\n",
    "\n",
    "# @pytest.fixture  (yield fixture with teardown)\n",
    "def temp_file_fixture() -> Generator[str, None, None]:\n",
    "    \"\"\"Provide a temporary file path; clean up after test.\"\"\"\n",
    "    fd, path = tempfile.mkstemp(suffix=\".txt\")\n",
    "    os.close(fd)\n",
    "    print(f\"    [Setup] Created temp file: {path}\")\n",
    "\n",
    "    yield path  # This is where the test runs\n",
    "\n",
    "    # Teardown: runs even if the test fails\n",
    "    if os.path.exists(path):\n",
    "        os.unlink(path)\n",
    "        print(f\"    [Teardown] Removed temp file: {path}\")\n",
    "\n",
    "\n",
    "# --- Using fixtures in tests ---\n",
    "# In real pytest, fixtures are injected as function parameters:\n",
    "#   def test_add(calculator: Calculator) -> None:\n",
    "#       assert calculator.add(1, 2) == 3\n",
    "\n",
    "\n",
    "# Demonstrate fixture lifecycle\n",
    "print(\"=== Fixture lifecycle demo ===\")\n",
    "print(\"\\n--- Fresh calculator fixture ---\")\n",
    "calc = calculator_fixture()\n",
    "print(f\"  History: {calc.history} (empty, fresh instance)\")\n",
    "calc.add(5, 10)\n",
    "print(f\"  After add: {calc.history}\")\n",
    "\n",
    "print(\"\\n--- Preloaded calculator fixture ---\")\n",
    "calc = preloaded_calculator_fixture()\n",
    "print(f\"  History: {calc.history} (pre-populated)\")\n",
    "\n",
    "print(\"\\n--- Yield fixture with teardown ---\")\n",
    "gen = temp_file_fixture()\n",
    "path = next(gen)  # Setup runs, we get the path\n",
    "print(f\"    [Test] Writing to {path}\")\n",
    "with open(path, \"w\") as f:\n",
    "    f.write(\"test data\")\n",
    "try:\n",
    "    next(gen)  # Trigger teardown\n",
    "except StopIteration:\n",
    "    pass\n",
    "\n",
    "print(f\"    File exists after teardown: {os.path.exists(path)}\")\n",
    "\n",
    "\n",
    "# --- Fixture scope illustration ---\n",
    "print(\"\\n=== Fixture scopes ===\")\n",
    "scopes = {\n",
    "    \"function\": \"New instance per test function (default, most isolated)\",\n",
    "    \"class\": \"Shared across all methods in a test class\",\n",
    "    \"module\": \"Shared across all tests in a module (file)\",\n",
    "    \"session\": \"Shared across the entire test session (all files)\",\n",
    "}\n",
    "for scope, description in scopes.items():\n",
    "    print(f\"  @pytest.fixture(scope='{scope}')\")\n",
    "    print(f\"    -> {description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Exception Testing with pytest.raises\n",
    "\n",
    "Testing that code raises the correct exception is just as important as testing\n",
    "the happy path. `pytest.raises` is a context manager that captures and\n",
    "inspects exceptions. Here we demonstrate the pattern manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "# In real pytest code:\n",
    "#   def test_divide_by_zero():\n",
    "#       calc = Calculator()\n",
    "#       with pytest.raises(DivisionByZeroError, match=\"Cannot divide\"):\n",
    "#           calc.divide(10, 0)\n",
    "\n",
    "\n",
    "def assert_raises(\n",
    "    exc_type: type[Exception],\n",
    "    callable_obj: object,\n",
    "    *args: object,\n",
    "    match: str | None = None,\n",
    ") -> Exception:\n",
    "    \"\"\"Simplified version of pytest.raises for demonstration.\"\"\"\n",
    "    try:\n",
    "        callable_obj(*args)  # type: ignore[operator]\n",
    "    except exc_type as e:\n",
    "        if match and not re.search(match, str(e)):\n",
    "            raise AssertionError(\n",
    "                f\"Exception message '{e}' did not match pattern '{match}'\"\n",
    "            ) from e\n",
    "        return e\n",
    "    except Exception as e:\n",
    "        raise AssertionError(\n",
    "            f\"Expected {exc_type.__name__}, got {type(e).__name__}: {e}\"\n",
    "        ) from e\n",
    "    else:\n",
    "        raise AssertionError(f\"Expected {exc_type.__name__} but no exception was raised\")\n",
    "\n",
    "\n",
    "# Test exception type\n",
    "calc = Calculator()\n",
    "exc = assert_raises(DivisionByZeroError, calc.divide, 10, 0)\n",
    "print(f\"PASSED: Caught {type(exc).__name__}: {exc}\")\n",
    "\n",
    "# Test exception message with regex match\n",
    "exc = assert_raises(DivisionByZeroError, calc.divide, 42, 0, match=r\"Cannot divide 42\")\n",
    "print(f\"PASSED: Message matched pattern: '{exc}'\")\n",
    "\n",
    "# Test exception inheritance (DivisionByZeroError is a CalculatorError)\n",
    "exc = assert_raises(CalculatorError, calc.divide, 1, 0)\n",
    "print(f\"PASSED: Caught via parent class: {type(exc).__name__}\")\n",
    "\n",
    "# Test that NO exception is raised when it should be\n",
    "try:\n",
    "    assert_raises(DivisionByZeroError, calc.divide, 10, 2)\n",
    "    print(\"FAILED: Should have reported missing exception\")\n",
    "except AssertionError as e:\n",
    "    print(f\"PASSED: Correctly detected missing exception: {e}\")\n",
    "\n",
    "# Test wrong exception type\n",
    "try:\n",
    "    assert_raises(TypeError, calc.divide, 10, 0)\n",
    "    print(\"FAILED: Should have reported wrong exception type\")\n",
    "except AssertionError as e:\n",
    "    print(f\"PASSED: Correctly detected wrong exception type: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## pytest.mark: Parametrize, Skip, and Expected Failures\n",
    "\n",
    "Markers add metadata to tests:\n",
    "- `@pytest.mark.parametrize`: Run the same test with different inputs\n",
    "- `@pytest.mark.skip` / `@pytest.mark.skipif`: Skip tests conditionally\n",
    "- `@pytest.mark.xfail`: Mark a test as expected to fail\n",
    "\n",
    "Parametrization is especially powerful -- it generates a separate test case\n",
    "for each set of inputs, so failures pinpoint exactly which case broke."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import platform\n",
    "\n",
    "\n",
    "# --- @pytest.mark.parametrize ---\n",
    "# In real pytest:\n",
    "#   @pytest.mark.parametrize(\"a, b, expected\", [\n",
    "#       (2, 3, 5),\n",
    "#       (-1, 1, 0),\n",
    "#       (0.1, 0.2, 0.3),\n",
    "#   ])\n",
    "#   def test_add_parametrized(calculator, a, b, expected):\n",
    "#       assert calculator.add(a, b) == pytest.approx(expected)\n",
    "\n",
    "\n",
    "def test_add_parametrized() -> None:\n",
    "    \"\"\"Demonstrate parametrized testing.\"\"\"\n",
    "    test_cases: list[tuple[float, float, float]] = [\n",
    "        (2, 3, 5),\n",
    "        (-1, 1, 0),\n",
    "        (0, 0, 0),\n",
    "        (100, -50, 50),\n",
    "        (0.1, 0.2, 0.3),\n",
    "        (-2.5, -3.5, -6.0),\n",
    "    ]\n",
    "\n",
    "    calc = Calculator()\n",
    "    for a, b, expected in test_cases:\n",
    "        result = calc.add(a, b)\n",
    "        passed = abs(result - expected) < 1e-9\n",
    "        status = \"PASSED\" if passed else \"FAILED\"\n",
    "        print(f\"  {status}: add({a}, {b}) == {expected} (got {result})\")\n",
    "\n",
    "\n",
    "# --- @pytest.mark.skip and @pytest.mark.skipif ---\n",
    "def test_skip_examples() -> None:\n",
    "    \"\"\"Demonstrate skip conditions.\"\"\"\n",
    "    # @pytest.mark.skip(reason=\"Not yet implemented\")\n",
    "    print(\"  SKIPPED: test_new_feature - reason: Not yet implemented\")\n",
    "\n",
    "    # @pytest.mark.skipif(sys.platform == \"win32\", reason=\"Unix only\")\n",
    "    if sys.platform == \"win32\":\n",
    "        print(\"  SKIPPED: test_unix_permissions - reason: Unix only\")\n",
    "    else:\n",
    "        print(f\"  RAN: test_unix_permissions (platform={sys.platform})\")\n",
    "\n",
    "    # @pytest.mark.skipif(sys.version_info < (3, 12), reason=\"Requires 3.12+\")\n",
    "    if sys.version_info < (3, 12):\n",
    "        print(f\"  SKIPPED: test_312_feature - reason: Requires 3.12+ (have {sys.version_info[:2]})\")\n",
    "    else:\n",
    "        print(f\"  RAN: test_312_feature (version={'.'.join(map(str, sys.version_info[:2]))})\")\n",
    "\n",
    "\n",
    "# --- @pytest.mark.xfail ---\n",
    "def test_xfail_example() -> None:\n",
    "    \"\"\"Demonstrate expected failure.\"\"\"\n",
    "    # @pytest.mark.xfail(reason=\"Known floating point precision issue\")\n",
    "    # The test is expected to fail, and that's OK\n",
    "    try:\n",
    "        assert 0.1 + 0.2 == 0.3  # This will fail\n",
    "        print(\"  XPASS: Float comparison unexpectedly passed (strict=True would fail)\")\n",
    "    except AssertionError:\n",
    "        print(\"  XFAIL: Float comparison failed as expected (known issue)\")\n",
    "\n",
    "\n",
    "print(\"=== Parametrized tests ===\")\n",
    "test_add_parametrized()\n",
    "print(\"\\n=== Skip/skipif examples ===\")\n",
    "test_skip_examples()\n",
    "print(\"\\n=== Expected failures (xfail) ===\")\n",
    "test_xfail_example()\n",
    "\n",
    "# --- What these look like in real pytest files ---\n",
    "print(\"\\n=== Real pytest syntax (for reference) ===\")\n",
    "real_pytest_example = '''\n",
    "import pytest\n",
    "\n",
    "@pytest.mark.parametrize(\"a, b, expected\", [\n",
    "    (2, 3, 5),\n",
    "    (-1, 1, 0),\n",
    "    (0.1, 0.2, pytest.approx(0.3)),\n",
    "])\n",
    "def test_add(calculator: Calculator, a: float, b: float, expected: float) -> None:\n",
    "    assert calculator.add(a, b) == expected\n",
    "\n",
    "@pytest.mark.skip(reason=\"Not yet implemented\")\n",
    "def test_future_feature() -> None:\n",
    "    ...\n",
    "\n",
    "@pytest.mark.xfail(reason=\"Known bug #1234\", strict=True)\n",
    "def test_known_bug() -> None:\n",
    "    ...\n",
    "'''\n",
    "print(real_pytest_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Mocking with unittest.mock\n",
    "\n",
    "Mocking replaces real objects with controllable stand-ins during testing.\n",
    "This is essential for isolating units from external dependencies\n",
    "(databases, APIs, file systems, time).\n",
    "\n",
    "Key tools:\n",
    "- `MagicMock`: A flexible mock object that accepts any attribute/call\n",
    "- `patch`: Temporarily replace an object in a specific namespace\n",
    "- `side_effect`: Make a mock raise exceptions or return different values per call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unittest.mock import MagicMock, patch, call\n",
    "from dataclasses import dataclass\n",
    "from typing import Protocol\n",
    "\n",
    "\n",
    "# --- Production code: a service that depends on external systems ---\n",
    "\n",
    "class DatabaseError(Exception):\n",
    "    \"\"\"Raised when a database operation fails.\"\"\"\n",
    "\n",
    "\n",
    "class UserRepository(Protocol):\n",
    "    \"\"\"Protocol defining the repository interface.\"\"\"\n",
    "    def get_user(self, user_id: int) -> dict[str, object]: ...\n",
    "    def save_user(self, user: dict[str, object]) -> bool: ...\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class UserService:\n",
    "    \"\"\"Service that uses a repository (to be mocked in tests).\"\"\"\n",
    "    repository: UserRepository\n",
    "\n",
    "    def get_user_display_name(self, user_id: int) -> str:\n",
    "        \"\"\"Get a formatted display name for a user.\"\"\"\n",
    "        user = self.repository.get_user(user_id)\n",
    "        first = user.get(\"first_name\", \"Unknown\")\n",
    "        last = user.get(\"last_name\", \"\")\n",
    "        return f\"{first} {last}\".strip()\n",
    "\n",
    "    def update_user_email(self, user_id: int, new_email: str) -> bool:\n",
    "        \"\"\"Update a user's email address.\"\"\"\n",
    "        user = self.repository.get_user(user_id)\n",
    "        user[\"email\"] = new_email\n",
    "        return self.repository.save_user(user)\n",
    "\n",
    "\n",
    "# --- MagicMock basics ---\n",
    "print(\"=== MagicMock basics ===\")\n",
    "mock_repo = MagicMock(spec=UserRepository)\n",
    "mock_repo.get_user.return_value = {\n",
    "    \"first_name\": \"Alice\",\n",
    "    \"last_name\": \"Smith\",\n",
    "    \"email\": \"alice@example.com\",\n",
    "}\n",
    "\n",
    "service = UserService(repository=mock_repo)\n",
    "display_name = service.get_user_display_name(user_id=42)\n",
    "\n",
    "print(f\"  Display name: {display_name}\")\n",
    "assert display_name == \"Alice Smith\"\n",
    "\n",
    "# Verify the mock was called correctly\n",
    "mock_repo.get_user.assert_called_once_with(42)\n",
    "print(f\"  get_user called with: {mock_repo.get_user.call_args}\")\n",
    "print(f\"  get_user call count: {mock_repo.get_user.call_count}\")\n",
    "\n",
    "\n",
    "# --- side_effect: simulate exceptions ---\n",
    "print(\"\\n=== side_effect for exceptions ===\")\n",
    "mock_repo_failing = MagicMock(spec=UserRepository)\n",
    "mock_repo_failing.get_user.side_effect = DatabaseError(\"Connection timeout\")\n",
    "\n",
    "service_failing = UserService(repository=mock_repo_failing)\n",
    "try:\n",
    "    service_failing.get_user_display_name(1)\n",
    "except DatabaseError as e:\n",
    "    print(f\"  Caught expected error: {e}\")\n",
    "\n",
    "\n",
    "# --- side_effect: different return values per call ---\n",
    "print(\"\\n=== side_effect for sequential returns ===\")\n",
    "mock_repo_sequence = MagicMock(spec=UserRepository)\n",
    "mock_repo_sequence.get_user.side_effect = [\n",
    "    {\"first_name\": \"Alice\", \"last_name\": \"Smith\"},\n",
    "    {\"first_name\": \"Bob\"},  # No last name\n",
    "    DatabaseError(\"Server down\"),  # Third call raises\n",
    "]\n",
    "\n",
    "service_seq = UserService(repository=mock_repo_sequence)\n",
    "print(f\"  Call 1: {service_seq.get_user_display_name(1)}\")\n",
    "print(f\"  Call 2: {service_seq.get_user_display_name(2)}\")\n",
    "try:\n",
    "    service_seq.get_user_display_name(3)\n",
    "except DatabaseError as e:\n",
    "    print(f\"  Call 3: Raised {e}\")\n",
    "\n",
    "# Verify all calls\n",
    "expected_calls = [call(1), call(2), call(3)]\n",
    "mock_repo_sequence.get_user.assert_has_calls(expected_calls)\n",
    "print(f\"  All expected calls verified\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Mocking with patch: Replacing Module-Level Objects\n",
    "\n",
    "`patch` temporarily replaces an object at its lookup location. This is\n",
    "critical for testing code that calls `time.time()`, `os.path.exists()`,\n",
    "or other module-level functions. The key rule: **patch where the object\n",
    "is looked up, not where it is defined**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unittest.mock import patch, MagicMock\n",
    "import time\n",
    "import os\n",
    "\n",
    "\n",
    "# --- Production code that depends on time and filesystem ---\n",
    "\n",
    "def get_uptime_message() -> str:\n",
    "    \"\"\"Return a message with the current timestamp.\"\"\"\n",
    "    current = time.time()\n",
    "    return f\"System checked at {current:.0f}\"\n",
    "\n",
    "\n",
    "def read_config(path: str) -> dict[str, str]:\n",
    "    \"\"\"Read a config file if it exists, else return defaults.\"\"\"\n",
    "    if os.path.exists(path):\n",
    "        # In real code, we'd read and parse the file\n",
    "        return {\"source\": \"file\", \"path\": path}\n",
    "    return {\"source\": \"defaults\", \"debug\": \"false\"}\n",
    "\n",
    "\n",
    "# --- patch as context manager ---\n",
    "print(\"=== patch as context manager ===\")\n",
    "with patch(\"time.time\", return_value=1700000000.0):\n",
    "    message = get_uptime_message()\n",
    "    print(f\"  {message}\")\n",
    "    assert \"1700000000\" in message\n",
    "\n",
    "# After the with block, time.time() is restored\n",
    "print(f\"  Real time after patch: {time.time():.0f}\")\n",
    "\n",
    "\n",
    "# --- patch for filesystem operations ---\n",
    "print(\"\\n=== Patching os.path.exists ===\")\n",
    "\n",
    "# Simulate file exists\n",
    "with patch(\"os.path.exists\", return_value=True):\n",
    "    config = read_config(\"/etc/myapp/config.yaml\")\n",
    "    print(f\"  File 'exists': {config}\")\n",
    "    assert config[\"source\"] == \"file\"\n",
    "\n",
    "# Simulate file missing\n",
    "with patch(\"os.path.exists\", return_value=False):\n",
    "    config = read_config(\"/etc/myapp/config.yaml\")\n",
    "    print(f\"  File 'missing': {config}\")\n",
    "    assert config[\"source\"] == \"defaults\"\n",
    "\n",
    "\n",
    "# --- patch as decorator (shown as reference) ---\n",
    "print(\"\\n=== patch decorator syntax (for test files) ===\")\n",
    "reference = '''\n",
    "# Patch where the object is USED, not where it's defined:\n",
    "\n",
    "@patch(\"myapp.services.time.time\", return_value=1700000000.0)\n",
    "def test_uptime_message(mock_time: MagicMock) -> None:\n",
    "    message = get_uptime_message()\n",
    "    assert \"1700000000\" in message\n",
    "    mock_time.assert_called_once()\n",
    "\n",
    "# Multiple patches (applied bottom-up, passed left-to-right):\n",
    "@patch(\"myapp.services.os.path.exists\", return_value=True)\n",
    "@patch(\"myapp.services.open\", create=True)\n",
    "def test_read_config(mock_open: MagicMock, mock_exists: MagicMock) -> None:\n",
    "    config = read_config(\"/etc/config.yaml\")\n",
    "    mock_exists.assert_called_once_with(\"/etc/config.yaml\")\n",
    "'''\n",
    "print(reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Testing Patterns: Complete Test Suite Example\n",
    "\n",
    "A well-organized test suite follows consistent patterns. Below is what a\n",
    "complete `conftest.py` and test file would look like for our Calculator,\n",
    "demonstrating fixture sharing, parametrization, and clean AAA structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === conftest.py (shared fixtures) ===\n",
    "conftest_example = '''\n",
    "# conftest.py - pytest discovers this automatically\n",
    "import pytest\n",
    "from myapp.calculator import Calculator\n",
    "\n",
    "@pytest.fixture\n",
    "def calculator() -> Calculator:\n",
    "    \"\"\"Provide a fresh Calculator for each test.\"\"\"\n",
    "    return Calculator(precision=2)\n",
    "\n",
    "@pytest.fixture\n",
    "def calculator_with_history(calculator: Calculator) -> Calculator:\n",
    "    \"\"\"Calculator with pre-populated history.\"\"\"\n",
    "    calculator.add(10, 20)\n",
    "    calculator.multiply(3, 7)\n",
    "    return calculator\n",
    "'''\n",
    "\n",
    "# === test_calculator.py ===\n",
    "test_file_example = '''\n",
    "import pytest\n",
    "from myapp.calculator import Calculator, DivisionByZeroError\n",
    "\n",
    "\n",
    "class TestCalculatorArithmetic:\n",
    "    \"\"\"Tests for basic arithmetic operations.\"\"\"\n",
    "\n",
    "    @pytest.mark.parametrize(\"a, b, expected\", [\n",
    "        (2, 3, 5),\n",
    "        (-1, 1, 0),\n",
    "        (0, 0, 0),\n",
    "        (1.5, 2.5, 4.0),\n",
    "    ])\n",
    "    def test_add(self, calculator: Calculator, a: float, b: float, expected: float) -> None:\n",
    "        assert calculator.add(a, b) == pytest.approx(expected)\n",
    "\n",
    "    @pytest.mark.parametrize(\"a, b, expected\", [\n",
    "        (10, 3, 3.33),\n",
    "        (1, 3, 0.33),\n",
    "        (-6, 2, -3.0),\n",
    "    ])\n",
    "    def test_divide(self, calculator: Calculator, a: float, b: float, expected: float) -> None:\n",
    "        assert calculator.divide(a, b) == pytest.approx(expected)\n",
    "\n",
    "    def test_divide_by_zero(self, calculator: Calculator) -> None:\n",
    "        with pytest.raises(DivisionByZeroError, match=\"Cannot divide\"):\n",
    "            calculator.divide(10, 0)\n",
    "\n",
    "\n",
    "class TestCalculatorHistory:\n",
    "    \"\"\"Tests for history tracking.\"\"\"\n",
    "\n",
    "    def test_history_starts_empty(self, calculator: Calculator) -> None:\n",
    "        assert calculator.history == []\n",
    "\n",
    "    def test_operations_are_recorded(\n",
    "        self, calculator_with_history: Calculator\n",
    "    ) -> None:\n",
    "        assert len(calculator_with_history.history) == 2\n",
    "\n",
    "    def test_clear_history(\n",
    "        self, calculator_with_history: Calculator\n",
    "    ) -> None:\n",
    "        calculator_with_history.clear_history()\n",
    "        assert calculator_with_history.history == []\n",
    "'''\n",
    "\n",
    "print(\"=== conftest.py ===\")\n",
    "print(conftest_example)\n",
    "print(\"=== test_calculator.py ===\")\n",
    "print(test_file_example)\n",
    "\n",
    "print(\"=== Running tests would produce output like: ===\")\n",
    "print(\"\"\"\n",
    "$ pytest -v test_calculator.py\n",
    "\n",
    "test_calculator.py::TestCalculatorArithmetic::test_add[2-3-5]          PASSED\n",
    "test_calculator.py::TestCalculatorArithmetic::test_add[-1-1-0]         PASSED\n",
    "test_calculator.py::TestCalculatorArithmetic::test_add[0-0-0]          PASSED\n",
    "test_calculator.py::TestCalculatorArithmetic::test_add[1.5-2.5-4.0]   PASSED\n",
    "test_calculator.py::TestCalculatorArithmetic::test_divide[10-3-3.33]   PASSED\n",
    "test_calculator.py::TestCalculatorArithmetic::test_divide[1-3-0.33]    PASSED\n",
    "test_calculator.py::TestCalculatorArithmetic::test_divide[-6-2--3.0]   PASSED\n",
    "test_calculator.py::TestCalculatorArithmetic::test_divide_by_zero      PASSED\n",
    "test_calculator.py::TestCalculatorHistory::test_history_starts_empty   PASSED\n",
    "test_calculator.py::TestCalculatorHistory::test_operations_are_recorded PASSED\n",
    "test_calculator.py::TestCalculatorHistory::test_clear_history          PASSED\n",
    "\n",
    "========================= 11 passed in 0.03s =========================\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}