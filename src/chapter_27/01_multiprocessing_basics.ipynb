{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# Chapter 27: Multiprocessing Basics\n",
    "\n",
    "Python's `multiprocessing` module enables true parallelism by spawning separate operating\n",
    "system processes, each with its own Python interpreter and memory space. Unlike threads,\n",
    "processes are not constrained by the Global Interpreter Lock (GIL), making multiprocessing\n",
    "the go-to approach for CPU-bound workloads.\n",
    "\n",
    "## Topics Covered\n",
    "- **Process**: Creating and running child processes\n",
    "- **Process lifecycle**: `start()`, `join()`, `is_alive()`, exit codes\n",
    "- **Process identity**: PIDs and process names\n",
    "- **Pool**: Worker pools for parallel task execution\n",
    "- **Pool.map()**: Distributing work across processes\n",
    "- **Pool.apply_async()**: Non-blocking task submission\n",
    "- **Practical**: Comparing sequential vs parallel execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5",
   "metadata": {},
   "source": [
    "## The multiprocessing Module\n",
    "\n",
    "The `multiprocessing` module mirrors the `threading` API but uses **processes** instead of\n",
    "threads. Each child process gets its own memory space and Python interpreter, which means:\n",
    "\n",
    "- No GIL contention -- true parallel execution on multiple CPU cores\n",
    "- No shared state by default -- data must be explicitly passed or shared\n",
    "- Higher overhead than threads (process creation is more expensive)\n",
    "- Functions passed to processes must be **picklable** (defined at module level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d4e5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Basic information about the current environment\n",
    "print(f\"Python process PID: {os.getpid()}\")\n",
    "print(f\"CPU count: {os.cpu_count()}\")\n",
    "print(f\"Start method: {multiprocessing.get_start_method()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e5f6a7",
   "metadata": {},
   "source": [
    "## Creating a Process\n",
    "\n",
    "The `multiprocessing.Process` class creates a new process. You provide a **target** function\n",
    "and optional **args** or **kwargs**. The process does not start until you call `.start()`.\n",
    "\n",
    "Key methods:\n",
    "- `start()` -- spawn the child process and begin execution\n",
    "- `join(timeout=None)` -- block until the process finishes (or timeout expires)\n",
    "- `is_alive()` -- check if the process is still running\n",
    "- `terminate()` / `kill()` -- forcibly stop the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f6a7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import os\n",
    "\n",
    "\n",
    "def worker(name: str) -> None:\n",
    "    \"\"\"A simple worker function that runs in a child process.\"\"\"\n",
    "    pid: int = os.getpid()\n",
    "    parent_pid: int = os.getppid()\n",
    "    print(f\"Worker '{name}' running in PID {pid} (parent PID: {parent_pid})\")\n",
    "\n",
    "\n",
    "# Create and start a process\n",
    "p = multiprocessing.Process(target=worker, args=(\"alpha\",))\n",
    "print(f\"Before start: is_alive={p.is_alive()}, pid={p.pid}\")\n",
    "\n",
    "p.start()\n",
    "print(f\"After start:  is_alive={p.is_alive()}, pid={p.pid}\")\n",
    "\n",
    "p.join()  # Wait for the process to finish\n",
    "print(f\"After join:   is_alive={p.is_alive()}, exitcode={p.exitcode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a7b8c9",
   "metadata": {},
   "source": [
    "## Process Lifecycle and Exit Codes\n",
    "\n",
    "Every process has a lifecycle: created, started, running, and terminated. After a process\n",
    "finishes, you can inspect its `exitcode`:\n",
    "\n",
    "| Exit code | Meaning |\n",
    "|-----------|:--------|\n",
    "| `0` | Normal termination |\n",
    "| `> 0` | Exception occurred (exit code 1) |\n",
    "| `< 0` | Killed by signal `-N` (e.g., -9 = SIGKILL) |\n",
    "| `None` | Process has not yet terminated |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b8c9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import time\n",
    "\n",
    "\n",
    "def quick_task() -> None:\n",
    "    \"\"\"A task that finishes quickly.\"\"\"\n",
    "    time.sleep(0.1)\n",
    "\n",
    "\n",
    "def slow_task() -> None:\n",
    "    \"\"\"A task that takes longer.\"\"\"\n",
    "    time.sleep(2.0)\n",
    "\n",
    "\n",
    "# Demonstrate process lifecycle\n",
    "p = multiprocessing.Process(target=quick_task, name=\"QuickWorker\")\n",
    "print(f\"Created:  name={p.name}, alive={p.is_alive()}, exitcode={p.exitcode}\")\n",
    "\n",
    "p.start()\n",
    "print(f\"Started:  name={p.name}, alive={p.is_alive()}, exitcode={p.exitcode}\")\n",
    "\n",
    "p.join()\n",
    "print(f\"Finished: name={p.name}, alive={p.is_alive()}, exitcode={p.exitcode}\")\n",
    "\n",
    "# Demonstrate join with timeout\n",
    "p2 = multiprocessing.Process(target=slow_task, name=\"SlowWorker\")\n",
    "p2.start()\n",
    "p2.join(timeout=0.5)  # Wait at most 0.5 seconds\n",
    "print(f\"\\nAfter timeout: alive={p2.is_alive()}, exitcode={p2.exitcode}\")\n",
    "\n",
    "p2.terminate()  # Forcibly stop the process\n",
    "p2.join()       # Clean up\n",
    "print(f\"After terminate: alive={p2.is_alive()}, exitcode={p2.exitcode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c9d0e1",
   "metadata": {},
   "source": [
    "## Multiple Processes and Separate Memory\n",
    "\n",
    "Each process has its own memory space. Modifications to variables in a child process do\n",
    "**not** affect the parent process. This is fundamentally different from threading, where\n",
    "threads share the same memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d0e1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import os\n",
    "\n",
    "# Use a Queue to collect results from child processes\n",
    "# (since child processes have separate memory)\n",
    "\n",
    "\n",
    "def compute_square(n: int, result_queue: multiprocessing.Queue) -> None:\n",
    "    \"\"\"Compute a square and put the result in a queue.\"\"\"\n",
    "    pid: int = os.getpid()\n",
    "    result: int = n * n\n",
    "    result_queue.put((n, result, pid))\n",
    "\n",
    "\n",
    "# Create multiple processes\n",
    "queue: multiprocessing.Queue = multiprocessing.Queue()\n",
    "processes: list[multiprocessing.Process] = []\n",
    "\n",
    "for i in range(4):\n",
    "    p = multiprocessing.Process(target=compute_square, args=(i, queue))\n",
    "    processes.append(p)\n",
    "    p.start()\n",
    "\n",
    "# Wait for all processes to complete\n",
    "for p in processes:\n",
    "    p.join()\n",
    "\n",
    "# Collect results\n",
    "print(f\"Main process PID: {os.getpid()}\")\n",
    "print(\"\\nResults from child processes:\")\n",
    "while not queue.empty():\n",
    "    n, result, pid = queue.get()\n",
    "    print(f\"  {n}^2 = {result} (computed by PID {pid})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e1f2a3",
   "metadata": {},
   "source": [
    "## Process Names and Daemon Processes\n",
    "\n",
    "Processes can be named for easier debugging. A **daemon** process runs in the background\n",
    "and is automatically terminated when the main process exits.\n",
    "\n",
    "- Set `daemon=True` before calling `start()` to create a daemon process\n",
    "- Daemon processes cannot spawn child processes of their own\n",
    "- They are useful for background tasks that should not prevent program exit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f2a3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "def named_worker() -> None:\n",
    "    \"\"\"Worker that prints its own name and PID.\"\"\"\n",
    "    proc = multiprocessing.current_process()\n",
    "    print(f\"  Process name={proc.name}, PID={os.getpid()}, daemon={proc.daemon}\")\n",
    "\n",
    "\n",
    "# Named processes\n",
    "p1 = multiprocessing.Process(target=named_worker, name=\"Worker-A\")\n",
    "p2 = multiprocessing.Process(target=named_worker, name=\"Worker-B\")\n",
    "\n",
    "p1.start()\n",
    "p2.start()\n",
    "p1.join()\n",
    "p2.join()\n",
    "\n",
    "# Daemon process example\n",
    "print(\"\\nDaemon process:\")\n",
    "p3 = multiprocessing.Process(target=named_worker, name=\"Daemon-Worker\", daemon=True)\n",
    "p3.start()\n",
    "p3.join(timeout=2)  # Must join daemon processes explicitly if you want to wait\n",
    "print(f\"  p3 daemon={p3.daemon}, exitcode={p3.exitcode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a3b4c5",
   "metadata": {},
   "source": [
    "## Pool: Managing Worker Processes\n",
    "\n",
    "Creating individual `Process` objects works for a few tasks, but for many tasks you want a\n",
    "**pool** of reusable worker processes. `multiprocessing.Pool` manages a fixed number of\n",
    "worker processes and distributes tasks to them.\n",
    "\n",
    "Key methods:\n",
    "- `map(func, iterable)` -- apply `func` to every item, return ordered results (blocking)\n",
    "- `starmap(func, iterable)` -- like `map()` but unpacks argument tuples\n",
    "- `apply(func, args)` -- call `func` with `args` in a worker (blocking)\n",
    "- `apply_async(func, args)` -- non-blocking version, returns `AsyncResult`\n",
    "- `map_async(func, iterable)` -- non-blocking version of `map()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b4c5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import os\n",
    "\n",
    "\n",
    "def square(x: int) -> int:\n",
    "    \"\"\"Compute the square of a number.\"\"\"\n",
    "    return x * x\n",
    "\n",
    "\n",
    "# Pool.map() distributes work across a pool of workers\n",
    "with multiprocessing.Pool(processes=2) as pool:\n",
    "    results: list[int] = pool.map(square, [1, 2, 3, 4, 5, 6, 7, 8])\n",
    "\n",
    "print(f\"Input:   {list(range(1, 9))}\")\n",
    "print(f\"Squared: {results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c5d6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "\n",
    "def power(base: int, exp: int) -> int:\n",
    "    \"\"\"Raise base to the given exponent.\"\"\"\n",
    "    return base ** exp\n",
    "\n",
    "\n",
    "# Pool.starmap() unpacks tuples of arguments\n",
    "args: list[tuple[int, int]] = [(2, 3), (3, 3), (4, 2), (5, 2), (10, 3)]\n",
    "\n",
    "with multiprocessing.Pool(processes=2) as pool:\n",
    "    results: list[int] = pool.starmap(power, args)\n",
    "\n",
    "for (base, exp), result in zip(args, results):\n",
    "    print(f\"  {base}^{exp} = {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d6e7f8",
   "metadata": {},
   "source": [
    "## Pool.apply_async(): Non-Blocking Execution\n",
    "\n",
    "`apply_async()` submits a single task to the pool without blocking. It returns an\n",
    "`AsyncResult` object that you can use to check status and retrieve results later.\n",
    "\n",
    "- `result.get(timeout=None)` -- block until the result is ready\n",
    "- `result.ready()` -- check if the result is available\n",
    "- `result.successful()` -- check if the task completed without error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e7f8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import time\n",
    "\n",
    "\n",
    "def slow_square(x: int) -> int:\n",
    "    \"\"\"Compute a square with a simulated delay.\"\"\"\n",
    "    time.sleep(0.3)\n",
    "    return x * x\n",
    "\n",
    "\n",
    "with multiprocessing.Pool(processes=2) as pool:\n",
    "    # Submit tasks asynchronously\n",
    "    async_results = [\n",
    "        pool.apply_async(slow_square, args=(i,))\n",
    "        for i in range(1, 6)\n",
    "    ]\n",
    "\n",
    "    # Check status while tasks are running\n",
    "    print(\"Submitted 5 tasks...\")\n",
    "    time.sleep(0.1)\n",
    "    for i, ar in enumerate(async_results, 1):\n",
    "        print(f\"  Task {i}: ready={ar.ready()}\")\n",
    "\n",
    "    # Collect results (blocks until ready)\n",
    "    print(\"\\nCollecting results:\")\n",
    "    for i, ar in enumerate(async_results, 1):\n",
    "        result: int = ar.get(timeout=5)\n",
    "        print(f\"  {i}^2 = {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f8a9b0",
   "metadata": {},
   "source": [
    "## Separate Process IDs\n",
    "\n",
    "A key characteristic of multiprocessing is that each worker runs in a separate OS process\n",
    "with its own PID. This is what gives us true parallelism, but it also means that data\n",
    "must be serialized (pickled) to pass between processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a9b0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import os\n",
    "\n",
    "\n",
    "def get_pid(_: int = 0) -> int:\n",
    "    \"\"\"Return the current process ID.\"\"\"\n",
    "    return os.getpid()\n",
    "\n",
    "\n",
    "main_pid: int = os.getpid()\n",
    "print(f\"Main process PID: {main_pid}\")\n",
    "\n",
    "with multiprocessing.Pool(processes=3) as pool:\n",
    "    pids: list[int] = pool.map(get_pid, range(6))\n",
    "\n",
    "print(f\"Worker PIDs:      {pids}\")\n",
    "unique_pids: set[int] = set(pids)\n",
    "print(f\"Unique workers:   {unique_pids}\")\n",
    "print(f\"\\nAll worker PIDs differ from main: {all(pid != main_pid for pid in pids)}\")\n",
    "print(f\"Number of unique workers: {len(unique_pids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b0c1d2",
   "metadata": {},
   "source": [
    "## Practical: Sequential vs Parallel Execution\n",
    "\n",
    "Let us compare the execution time of a CPU-bound workload run sequentially versus\n",
    "in parallel using `Pool.map()`. This demonstrates the real benefit of multiprocessing\n",
    "for CPU-intensive tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c1d2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import time\n",
    "\n",
    "\n",
    "def cpu_bound_task(n: int) -> int:\n",
    "    \"\"\"A CPU-bound task: sum of squares up to n.\"\"\"\n",
    "    total: int = 0\n",
    "    for i in range(n):\n",
    "        total += i * i\n",
    "    return total\n",
    "\n",
    "\n",
    "workload: list[int] = [2_000_000] * 8\n",
    "\n",
    "# Sequential execution\n",
    "start: float = time.perf_counter()\n",
    "sequential_results: list[int] = [cpu_bound_task(n) for n in workload]\n",
    "sequential_time: float = time.perf_counter() - start\n",
    "\n",
    "# Parallel execution\n",
    "start = time.perf_counter()\n",
    "with multiprocessing.Pool(processes=4) as pool:\n",
    "    parallel_results: list[int] = pool.map(cpu_bound_task, workload)\n",
    "parallel_time: float = time.perf_counter() - start\n",
    "\n",
    "print(f\"Sequential time: {sequential_time:.3f}s\")\n",
    "print(f\"Parallel time:   {parallel_time:.3f}s\")\n",
    "print(f\"Speedup:         {sequential_time / parallel_time:.2f}x\")\n",
    "print(f\"Results match:   {sequential_results == parallel_results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d2e3f4",
   "metadata": {},
   "source": [
    "## Error Handling in Processes\n",
    "\n",
    "When a child process raises an exception, the behavior depends on how you launched it:\n",
    "\n",
    "- **Process**: The exception is raised in the child only; the parent sees a non-zero `exitcode`\n",
    "- **Pool.map()**: The exception is re-raised in the parent when results are collected\n",
    "- **Pool.apply_async()**: The exception is re-raised when you call `.get()` on the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e3f4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "\n",
    "def risky_task(x: int) -> float:\n",
    "    \"\"\"A task that might fail.\"\"\"\n",
    "    if x == 0:\n",
    "        raise ValueError(\"Cannot process zero!\")\n",
    "    return 100.0 / x\n",
    "\n",
    "\n",
    "# Error with Pool.map() -- exception propagates to parent\n",
    "print(\"Testing Pool.map() error handling:\")\n",
    "try:\n",
    "    with multiprocessing.Pool(2) as pool:\n",
    "        pool.map(risky_task, [5, 2, 0, 1])  # 0 will cause an error\n",
    "except ValueError as e:\n",
    "    print(f\"  Caught in parent: {e}\")\n",
    "\n",
    "# Error with apply_async() -- exception on .get()\n",
    "print(\"\\nTesting apply_async() error handling:\")\n",
    "with multiprocessing.Pool(2) as pool:\n",
    "    future = pool.apply_async(risky_task, args=(0,))\n",
    "    try:\n",
    "        result = future.get(timeout=5)\n",
    "    except ValueError as e:\n",
    "        print(f\"  Caught from future.get(): {e}\")\n",
    "    print(f\"  future.successful(): {future.successful()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f4a5b6",
   "metadata": {},
   "source": [
    "## Pool with Initializer\n",
    "\n",
    "You can pass an `initializer` function to `Pool()` that runs once in each worker process\n",
    "when it starts. This is useful for setting up per-worker state (database connections,\n",
    "loading large data, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a5b6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import os\n",
    "\n",
    "# Module-level variable that will be set by the initializer\n",
    "worker_id: int = -1\n",
    "\n",
    "\n",
    "def init_worker(base_id: int) -> None:\n",
    "    \"\"\"Initialize each worker with a unique ID.\"\"\"\n",
    "    global worker_id\n",
    "    worker_id = base_id + os.getpid() % 100\n",
    "    print(f\"  Worker initialized: PID={os.getpid()}, worker_id={worker_id}\")\n",
    "\n",
    "\n",
    "def task_with_state(x: int) -> str:\n",
    "    \"\"\"A task that uses worker-local state.\"\"\"\n",
    "    return f\"worker_id={worker_id}, PID={os.getpid()}, input={x}, result={x*x}\"\n",
    "\n",
    "\n",
    "print(\"Initializing pool with per-worker setup:\")\n",
    "with multiprocessing.Pool(processes=2, initializer=init_worker, initargs=(1000,)) as pool:\n",
    "    results: list[str] = pool.map(task_with_state, range(4))\n",
    "\n",
    "print(\"\\nResults:\")\n",
    "for r in results:\n",
    "    print(f\"  {r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b6c7d8",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "| Concept | API | Purpose |\n",
    "|---------|-----|:--------|\n",
    "| **Process** | `multiprocessing.Process` | Spawn a single child process |\n",
    "| **Lifecycle** | `start()`, `join()`, `is_alive()` | Control process execution |\n",
    "| **Exit codes** | `p.exitcode` | `0`=success, `>0`=error, `<0`=signal |\n",
    "| **Daemon** | `Process(daemon=True)` | Background process, auto-killed on exit |\n",
    "| **Pool** | `multiprocessing.Pool(n)` | Manage a pool of `n` worker processes |\n",
    "| **map()** | `pool.map(func, iterable)` | Distribute work, return ordered results |\n",
    "| **starmap()** | `pool.starmap(func, args)` | Like map but unpacks argument tuples |\n",
    "| **apply_async()** | `pool.apply_async(func, args)` | Non-blocking single task submission |\n",
    "| **Initializer** | `Pool(initializer=func)` | Run setup code once per worker |\n",
    "\n",
    "### Best Practices\n",
    "- Always use `Pool` as a context manager (`with` statement) to ensure workers are cleaned up\n",
    "- Define worker functions at module level so they can be pickled\n",
    "- Use `join()` to wait for processes and avoid zombie processes\n",
    "- Prefer `Pool.map()` for batch processing over manually managing `Process` objects\n",
    "- Handle exceptions from workers -- they propagate when you collect results\n",
    "- Choose the number of pool workers based on `os.cpu_count()` for CPU-bound tasks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}