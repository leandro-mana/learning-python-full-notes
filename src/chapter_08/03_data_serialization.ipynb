{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Data Serialization\n",
    "\n",
    "**Chapter 8 - Learning Python, 5th Edition**\n",
    "\n",
    "Serialization converts Python objects into a format that can be stored or transmitted,\n",
    "then reconstructed later. Python provides built-in support for JSON (human-readable,\n",
    "cross-language), CSV (tabular data), pickle (Python-native), and struct (binary\n",
    "packing). Choosing the right format depends on interoperability, readability,\n",
    "performance, and security requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Section 1: JSON - Human-Readable, Cross-Language\n",
    "\n",
    "JSON (JavaScript Object Notation) is the most common serialization format for\n",
    "web APIs and configuration files. Python's `json` module maps between JSON types\n",
    "and Python types:\n",
    "\n",
    "| JSON | Python |\n",
    "|------|--------|\n",
    "| object | `dict` |\n",
    "| array | `list` |\n",
    "| string | `str` |\n",
    "| number (int) | `int` |\n",
    "| number (real) | `float` |\n",
    "| true/false | `True`/`False` |\n",
    "| null | `None` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "\n",
    "work_dir = Path(tempfile.mkdtemp(prefix=\"ch08_serial_\"))\n",
    "\n",
    "# Basic serialization: dumps() -> string, dump() -> file\n",
    "data: dict[str, object] = {\n",
    "    \"name\": \"Learning Python\",\n",
    "    \"edition\": 5,\n",
    "    \"price\": 59.99,\n",
    "    \"in_print\": True,\n",
    "    \"topics\": [\"types\", \"functions\", \"classes\", \"modules\"],\n",
    "    \"metadata\": None,\n",
    "}\n",
    "\n",
    "# Serialize to string\n",
    "json_str: str = json.dumps(data, indent=2)\n",
    "print(f\"json.dumps() output:\\n{json_str}\")\n",
    "\n",
    "# Deserialize from string\n",
    "restored: dict = json.loads(json_str)\n",
    "print(f\"\\njson.loads() type: {type(restored).__name__}\")\n",
    "print(f\"Round-trip equal: {data == restored}\")\n",
    "\n",
    "# Serialize to file\n",
    "json_file = work_dir / \"book.json\"\n",
    "with open(json_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Deserialize from file\n",
    "with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    from_file: dict = json.load(f)\n",
    "\n",
    "print(f\"\\nFile round-trip equal: {data == from_file}\")\n",
    "\n",
    "# Useful options\n",
    "compact: str = json.dumps(data, separators=(\",\", \":\"))\n",
    "print(f\"\\nCompact (no spaces): {compact[:60]}...\")\n",
    "\n",
    "sorted_keys: str = json.dumps(data, sort_keys=True, indent=2)\n",
    "print(f\"\\nSorted keys:\\n{sorted_keys}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Section 2: JSON Custom Encoders and Decoders\n",
    "\n",
    "JSON only supports basic types. For `datetime`, `dataclass`, `set`, `Path`, and\n",
    "other Python types, you need custom encoders and decoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Event:\n",
    "    \"\"\"An event with a date and attendees.\"\"\"\n",
    "    title: str\n",
    "    date: date\n",
    "    attendees: set[str]\n",
    "    location: Path | None = None\n",
    "\n",
    "\n",
    "# Custom encoder: handles types JSON doesn't natively support\n",
    "class EnhancedEncoder(json.JSONEncoder):\n",
    "    \"\"\"JSON encoder that handles datetime, set, Path, and dataclasses.\"\"\"\n",
    "\n",
    "    def default(self, obj: Any) -> Any:\n",
    "        if isinstance(obj, (datetime, date)):\n",
    "            return {\"__type__\": \"datetime\", \"iso\": obj.isoformat()}\n",
    "        if isinstance(obj, set):\n",
    "            return {\"__type__\": \"set\", \"items\": sorted(obj)}\n",
    "        if isinstance(obj, Path):\n",
    "            return {\"__type__\": \"Path\", \"path\": str(obj)}\n",
    "        if hasattr(obj, \"__dataclass_fields__\"):\n",
    "            return {\"__type__\": type(obj).__name__, **asdict(obj)}\n",
    "        return super().default(obj)\n",
    "\n",
    "\n",
    "# Custom decoder hook: reconstructs Python objects from JSON\n",
    "def enhanced_decoder(dct: dict) -> Any:\n",
    "    \"\"\"Object hook that reconstructs custom types from JSON.\"\"\"\n",
    "    type_tag = dct.get(\"__type__\")\n",
    "    if type_tag == \"datetime\":\n",
    "        return date.fromisoformat(dct[\"iso\"])\n",
    "    if type_tag == \"set\":\n",
    "        return set(dct[\"items\"])\n",
    "    if type_tag == \"Path\":\n",
    "        return Path(dct[\"path\"])\n",
    "    if type_tag == \"Event\":\n",
    "        return Event(\n",
    "            title=dct[\"title\"],\n",
    "            date=dct[\"date\"],\n",
    "            attendees=dct[\"attendees\"],\n",
    "            location=dct.get(\"location\"),\n",
    "        )\n",
    "    return dct\n",
    "\n",
    "\n",
    "# Create an Event with non-JSON-native types\n",
    "event = Event(\n",
    "    title=\"Python Meetup\",\n",
    "    date=date(2024, 6, 15),\n",
    "    attendees={\"Alice\", \"Bob\", \"Charlie\"},\n",
    "    location=Path(\"/home/user/events\"),\n",
    ")\n",
    "\n",
    "# Encode\n",
    "encoded: str = json.dumps(event, cls=EnhancedEncoder, indent=2)\n",
    "print(f\"Encoded Event:\\n{encoded}\")\n",
    "\n",
    "# Decode\n",
    "decoded: Event = json.loads(encoded, object_hook=enhanced_decoder)\n",
    "print(f\"\\nDecoded type: {type(decoded).__name__}\")\n",
    "print(f\"Decoded:      {decoded}\")\n",
    "print(f\"Date type:    {type(decoded.date).__name__}\")\n",
    "print(f\"Attendees:    {type(decoded.attendees).__name__} = {decoded.attendees}\")\n",
    "print(f\"Location:     {type(decoded.location).__name__} = {decoded.location}\")\n",
    "\n",
    "# Alternative: using the default= parameter (simpler for one-off use)\n",
    "simple_data = {\"timestamp\": datetime.now(), \"values\": {1, 2, 3}}\n",
    "\n",
    "def quick_serializer(obj: Any) -> Any:\n",
    "    if isinstance(obj, datetime):\n",
    "        return obj.isoformat()\n",
    "    if isinstance(obj, set):\n",
    "        return list(obj)\n",
    "    raise TypeError(f\"Not serializable: {type(obj)}\")\n",
    "\n",
    "quick_json = json.dumps(simple_data, default=quick_serializer)\n",
    "print(f\"\\nQuick serialize: {quick_json}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Section 3: CSV - Tabular Data\n",
    "\n",
    "CSV (Comma-Separated Values) is ubiquitous for tabular data exchange. Python's\n",
    "`csv` module handles quoting, escaping, and dialect differences.\n",
    "\n",
    "- `csv.reader` / `csv.writer` - work with lists of values\n",
    "- `csv.DictReader` / `csv.DictWriter` - work with dictionaries (named columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from io import StringIO\n",
    "\n",
    "# --- csv.writer / csv.reader ---\n",
    "csv_file = work_dir / \"products.csv\"\n",
    "\n",
    "products: list[list[str | float]] = [\n",
    "    [\"id\", \"name\", \"price\", \"category\"],\n",
    "    [1, \"Widget\", 9.99, \"Hardware\"],\n",
    "    [2, \"Gadget\", 24.99, \"Electronics\"],\n",
    "    [3, 'Cable, 6ft \"USB-C\"', 12.50, \"Accessories\"],  # Commas and quotes in data\n",
    "    [4, \"Battery Pack\", 39.99, \"Electronics\"],\n",
    "]\n",
    "\n",
    "with open(csv_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(products)\n",
    "\n",
    "# Read back with csv.reader\n",
    "with open(csv_file, \"r\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    header: list[str] = next(reader)\n",
    "    rows: list[list[str]] = list(reader)\n",
    "\n",
    "print(f\"Header: {header}\")\n",
    "for row in rows:\n",
    "    print(f\"  {row}\")\n",
    "\n",
    "# Note: csv.reader returns all values as strings\n",
    "print(f\"\\nPrice type from csv.reader: {type(rows[0][2]).__name__} (always str)\")\n",
    "\n",
    "# --- csv.DictWriter / csv.DictReader (named columns) ---\n",
    "employees_file = work_dir / \"employees.csv\"\n",
    "\n",
    "employees: list[dict[str, str | int]] = [\n",
    "    {\"name\": \"Alice\", \"department\": \"Engineering\", \"salary\": 95000},\n",
    "    {\"name\": \"Bob\", \"department\": \"Marketing\", \"salary\": 72000},\n",
    "    {\"name\": \"Charlie\", \"department\": \"Engineering\", \"salary\": 88000},\n",
    "    {\"name\": \"Diana\", \"department\": \"Sales\", \"salary\": 68000},\n",
    "]\n",
    "\n",
    "fieldnames: list[str] = [\"name\", \"department\", \"salary\"]\n",
    "\n",
    "with open(employees_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(employees)\n",
    "\n",
    "# Read back with DictReader\n",
    "print(\"\\nDictReader output:\")\n",
    "with open(employees_file, \"r\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    print(f\"  Fieldnames: {reader.fieldnames}\")\n",
    "    for row in reader:\n",
    "        # row is an OrderedDict (or dict in 3.7+)\n",
    "        print(f\"  {row['name']:10s} {row['department']:15s} ${int(row['salary']):,}\")\n",
    "\n",
    "# CSV with different dialects\n",
    "tsv_data = \"name\\tage\\tcity\\nAlice\\t30\\tNew York\\nBob\\t25\\tLondon\\n\"\n",
    "reader = csv.reader(StringIO(tsv_data), delimiter=\"\\t\")\n",
    "print(\"\\nTSV (tab-separated):\")\n",
    "for row in reader:\n",
    "    print(f\"  {row}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Section 4: Pickle - Python-Native Serialization\n",
    "\n",
    "The `pickle` module serializes arbitrary Python objects to a binary format. It\n",
    "preserves object types, references, and structure. However, it comes with\n",
    "important caveats:\n",
    "\n",
    "**Security Warning**: Never unpickle data from untrusted sources. Pickle can execute\n",
    "arbitrary code during deserialization, making it a potential attack vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class UserSession:\n",
    "    \"\"\"A user session with complex nested data.\"\"\"\n",
    "    user_id: int\n",
    "    username: str\n",
    "    login_time: datetime\n",
    "    permissions: set[str]\n",
    "    preferences: dict[str, Any]\n",
    "\n",
    "\n",
    "session = UserSession(\n",
    "    user_id=42,\n",
    "    username=\"alice\",\n",
    "    login_time=datetime(2024, 1, 15, 10, 30, 0),\n",
    "    permissions={\"read\", \"write\", \"admin\"},\n",
    "    preferences={\n",
    "        \"theme\": \"dark\",\n",
    "        \"font_size\": 14,\n",
    "        \"recent_files\": [Path(\"/home/alice/doc.txt\"), Path(\"/tmp/data.csv\")],\n",
    "    },\n",
    ")\n",
    "\n",
    "# Serialize to bytes\n",
    "pickled: bytes = pickle.dumps(session)\n",
    "print(f\"Pickled size: {len(pickled)} bytes\")\n",
    "print(f\"Pickled type: {type(pickled).__name__}\")\n",
    "print(f\"First 50 bytes: {pickled[:50]!r}\")\n",
    "\n",
    "# Deserialize from bytes\n",
    "restored_session: UserSession = pickle.loads(pickled)\n",
    "print(f\"\\nRestored type: {type(restored_session).__name__}\")\n",
    "print(f\"Restored: {restored_session}\")\n",
    "print(f\"Permissions type: {type(restored_session.permissions).__name__}\")\n",
    "print(f\"Login time type: {type(restored_session.login_time).__name__}\")\n",
    "\n",
    "# Serialize to file\n",
    "pickle_file = work_dir / \"session.pkl\"\n",
    "with open(pickle_file, \"wb\") as f:\n",
    "    pickle.dump(session, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(pickle_file, \"rb\") as f:\n",
    "    from_file: UserSession = pickle.load(f)\n",
    "\n",
    "print(f\"\\nFile round-trip: {session == from_file}\")\n",
    "print(f\"Pickle protocol: {pickle.HIGHEST_PROTOCOL}\")\n",
    "\n",
    "# Pickle preserves object identity and circular references\n",
    "a_list: list[Any] = [1, 2, 3]\n",
    "circular: dict[str, Any] = {\"data\": a_list, \"same_data\": a_list}\n",
    "circular[\"self\"] = circular  # Circular reference!\n",
    "\n",
    "pickled_circular = pickle.dumps(circular)\n",
    "restored_circular = pickle.loads(pickled_circular)\n",
    "print(f\"\\nCircular reference preserved: {restored_circular['self'] is restored_circular}\")\n",
    "print(f\"Shared reference preserved: {restored_circular['data'] is restored_circular['same_data']}\")\n",
    "\n",
    "# SECURITY WARNING: demonstrate why pickle is dangerous\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"SECURITY WARNING:\")\n",
    "print(\"  pickle.loads() can execute arbitrary code.\")\n",
    "print(\"  NEVER unpickle data from untrusted sources.\")\n",
    "print(\"  Use JSON for data exchange with external systems.\")\n",
    "print(\"  Pickle is safe ONLY for data you created yourself.\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Section 5: The `struct` Module - Binary Data Packing\n",
    "\n",
    "The `struct` module converts between Python values and C-style binary data.\n",
    "This is essential for reading/writing binary file formats, network protocols,\n",
    "and hardware interfaces.\n",
    "\n",
    "Common format characters:\n",
    "\n",
    "| Format | C Type | Python Type | Size |\n",
    "|--------|--------|-------------|------|\n",
    "| `b`/`B` | signed/unsigned char | int | 1 |\n",
    "| `h`/`H` | short/unsigned short | int | 2 |\n",
    "| `i`/`I` | int/unsigned int | int | 4 |\n",
    "| `f` | float | float | 4 |\n",
    "| `d` | double | float | 8 |\n",
    "| `s` | char[] | bytes | n |\n",
    "\n",
    "Byte order prefixes: `<` little-endian, `>` big-endian, `!` network (big), `=` native"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "\n",
    "# Basic packing and unpacking\n",
    "# Format: int (4 bytes) + float (4 bytes) + bool (1 byte)\n",
    "fmt: str = \"<if?\"  # Little-endian: int, float, bool\n",
    "packed: bytes = struct.pack(fmt, 42, 3.14, True)\n",
    "print(f\"Format: {fmt!r}\")\n",
    "print(f\"Packed: {packed!r} ({len(packed)} bytes)\")\n",
    "print(f\"Calc size: {struct.calcsize(fmt)} bytes\")\n",
    "\n",
    "unpacked: tuple = struct.unpack(fmt, packed)\n",
    "print(f\"Unpacked: {unpacked}\")\n",
    "\n",
    "# Practical example: binary record format\n",
    "# Sensor data: timestamp (uint32), sensor_id (uint16), value (float32), status (uint8)\n",
    "SENSOR_FMT: str = \"!IHfB\"  # Network byte order (big-endian)\n",
    "SENSOR_SIZE: int = struct.calcsize(SENSOR_FMT)\n",
    "\n",
    "print(f\"\\nSensor record format: {SENSOR_FMT!r} ({SENSOR_SIZE} bytes per record)\")\n",
    "\n",
    "# Write sensor data to binary file\n",
    "sensor_records: list[tuple[int, int, float, int]] = [\n",
    "    (1705300000, 1, 23.5, 0),   # timestamp, sensor_id, value, status\n",
    "    (1705300060, 1, 23.7, 0),\n",
    "    (1705300120, 2, 45.2, 1),   # status=1 means warning\n",
    "    (1705300180, 1, 23.4, 0),\n",
    "    (1705300240, 2, 98.6, 2),   # status=2 means error\n",
    "]\n",
    "\n",
    "sensor_file = work_dir / \"sensors.bin\"\n",
    "with open(sensor_file, \"wb\") as f:\n",
    "    for record in sensor_records:\n",
    "        f.write(struct.pack(SENSOR_FMT, *record))\n",
    "\n",
    "# Read sensor data back\n",
    "print(f\"\\nReading {sensor_file.stat().st_size} bytes ({sensor_file.stat().st_size // SENSOR_SIZE} records):\")\n",
    "status_names = {0: \"OK\", 1: \"WARN\", 2: \"ERROR\"}\n",
    "\n",
    "with open(sensor_file, \"rb\") as f:\n",
    "    while True:\n",
    "        chunk: bytes = f.read(SENSOR_SIZE)\n",
    "        if not chunk:\n",
    "            break\n",
    "        ts, sid, val, status = struct.unpack(SENSOR_FMT, chunk)\n",
    "        dt = datetime.fromtimestamp(ts)\n",
    "        print(f\"  [{dt:%H:%M:%S}] sensor={sid} value={val:6.1f} status={status_names[status]}\")\n",
    "\n",
    "# Struct objects for repeated operations (faster than calling struct.pack/unpack)\n",
    "sensor_struct = struct.Struct(SENSOR_FMT)\n",
    "print(f\"\\nStruct object size: {sensor_struct.size} bytes\")\n",
    "print(f\"Struct format: {sensor_struct.format!r}\")\n",
    "\n",
    "packed_one = sensor_struct.pack(1705300300, 3, 72.1, 0)\n",
    "print(f\"Packed with Struct object: {packed_one.hex(' ')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Section 6: Practical Pattern - Multi-Format Configuration Loader\n",
    "\n",
    "A real-world utility that loads configuration from JSON, CSV, or INI files,\n",
    "auto-detecting the format from the file extension. This pattern demonstrates\n",
    "how to combine multiple serialization modules into a cohesive interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AppConfig:\n",
    "    \"\"\"Application configuration loaded from any supported format.\"\"\"\n",
    "    host: str = \"localhost\"\n",
    "    port: int = 8080\n",
    "    debug: bool = False\n",
    "    database_url: str = \"sqlite:///app.db\"\n",
    "    allowed_origins: list[str] = field(default_factory=lambda: [\"*\"])\n",
    "    max_connections: int = 100\n",
    "\n",
    "\n",
    "class ConfigLoader:\n",
    "    \"\"\"Load configuration from JSON, CSV, or INI files.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def load(path: Path) -> AppConfig:\n",
    "        \"\"\"Load config from file, auto-detecting format by extension.\"\"\"\n",
    "        suffix = path.suffix.lower()\n",
    "        loaders: dict[str, Any] = {\n",
    "            \".json\": ConfigLoader._load_json,\n",
    "            \".csv\": ConfigLoader._load_csv,\n",
    "            \".ini\": ConfigLoader._load_ini,\n",
    "        }\n",
    "        loader = loaders.get(suffix)\n",
    "        if loader is None:\n",
    "            raise ValueError(f\"Unsupported config format: {suffix}\")\n",
    "        return loader(path)\n",
    "\n",
    "    @staticmethod\n",
    "    def save_json(config: AppConfig, path: Path) -> None:\n",
    "        \"\"\"Save configuration as JSON.\"\"\"\n",
    "        data = {\n",
    "            \"host\": config.host,\n",
    "            \"port\": config.port,\n",
    "            \"debug\": config.debug,\n",
    "            \"database_url\": config.database_url,\n",
    "            \"allowed_origins\": config.allowed_origins,\n",
    "            \"max_connections\": config.max_connections,\n",
    "        }\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "\n",
    "    @staticmethod\n",
    "    def _load_json(path: Path) -> AppConfig:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        return AppConfig(**data)\n",
    "\n",
    "    @staticmethod\n",
    "    def _load_csv(path: Path) -> AppConfig:\n",
    "        \"\"\"Load config from a two-column CSV (key, value).\"\"\"\n",
    "        config_dict: dict[str, Any] = {}\n",
    "        with open(path, \"r\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                key, value = row[\"key\"], row[\"value\"]\n",
    "                # Parse types based on field annotations\n",
    "                if key in (\"port\", \"max_connections\"):\n",
    "                    config_dict[key] = int(value)\n",
    "                elif key == \"debug\":\n",
    "                    config_dict[key] = value.lower() in (\"true\", \"1\", \"yes\")\n",
    "                elif key == \"allowed_origins\":\n",
    "                    config_dict[key] = json.loads(value)\n",
    "                else:\n",
    "                    config_dict[key] = value\n",
    "        return AppConfig(**config_dict)\n",
    "\n",
    "    @staticmethod\n",
    "    def _load_ini(path: Path) -> AppConfig:\n",
    "        parser = configparser.ConfigParser()\n",
    "        parser.read(path, encoding=\"utf-8\")\n",
    "        section = parser[\"app\"] if \"app\" in parser else parser[parser.sections()[0]]\n",
    "        return AppConfig(\n",
    "            host=section.get(\"host\", \"localhost\"),\n",
    "            port=section.getint(\"port\", 8080),\n",
    "            debug=section.getboolean(\"debug\", False),\n",
    "            database_url=section.get(\"database_url\", \"sqlite:///app.db\"),\n",
    "            allowed_origins=json.loads(section.get(\"allowed_origins\", '[\"*\"]')),\n",
    "            max_connections=section.getint(\"max_connections\", 100),\n",
    "        )\n",
    "\n",
    "\n",
    "# Create configuration files in all three formats\n",
    "config = AppConfig(\n",
    "    host=\"0.0.0.0\",\n",
    "    port=9090,\n",
    "    debug=True,\n",
    "    database_url=\"postgresql://localhost/mydb\",\n",
    "    allowed_origins=[\"https://example.com\", \"https://api.example.com\"],\n",
    "    max_connections=50,\n",
    ")\n",
    "\n",
    "# Save as JSON\n",
    "json_path = work_dir / \"config.json\"\n",
    "ConfigLoader.save_json(config, json_path)\n",
    "\n",
    "# Create CSV config\n",
    "csv_path = work_dir / \"config.csv\"\n",
    "with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"key\", \"value\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerow({\"key\": \"host\", \"value\": \"0.0.0.0\"})\n",
    "    writer.writerow({\"key\": \"port\", \"value\": \"9090\"})\n",
    "    writer.writerow({\"key\": \"debug\", \"value\": \"true\"})\n",
    "    writer.writerow({\"key\": \"database_url\", \"value\": \"postgresql://localhost/mydb\"})\n",
    "    writer.writerow({\"key\": \"allowed_origins\",\n",
    "                     \"value\": json.dumps(config.allowed_origins)})\n",
    "    writer.writerow({\"key\": \"max_connections\", \"value\": \"50\"})\n",
    "\n",
    "# Create INI config\n",
    "ini_path = work_dir / \"config.ini\"\n",
    "ini_parser = configparser.ConfigParser()\n",
    "ini_parser[\"app\"] = {\n",
    "    \"host\": \"0.0.0.0\",\n",
    "    \"port\": \"9090\",\n",
    "    \"debug\": \"true\",\n",
    "    \"database_url\": \"postgresql://localhost/mydb\",\n",
    "    \"allowed_origins\": json.dumps(config.allowed_origins),\n",
    "    \"max_connections\": \"50\",\n",
    "}\n",
    "with open(ini_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    ini_parser.write(f)\n",
    "\n",
    "# Load from each format and verify they produce the same result\n",
    "print(\"Loading configuration from multiple formats:\\n\")\n",
    "for path in [json_path, csv_path, ini_path]:\n",
    "    loaded = ConfigLoader.load(path)\n",
    "    print(f\"  {path.suffix:5s}: host={loaded.host}, port={loaded.port}, \"\n",
    "          f\"debug={loaded.debug}, origins={len(loaded.allowed_origins)}\")\n",
    "\n",
    "# Verify equivalence\n",
    "from_json = ConfigLoader.load(json_path)\n",
    "from_csv = ConfigLoader.load(csv_path)\n",
    "from_ini = ConfigLoader.load(ini_path)\n",
    "print(f\"\\nAll formats equivalent: {from_json == from_csv == from_ini}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Section 7: Comparing Serialization Formats\n",
    "\n",
    "Each format has trade-offs. Choosing the right one depends on your requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Compare serialization size and speed for the same data\n",
    "test_data: list[dict[str, Any]] = [\n",
    "    {\n",
    "        \"id\": i,\n",
    "        \"name\": f\"item_{i:04d}\",\n",
    "        \"value\": i * 1.5,\n",
    "        \"active\": i % 3 != 0,\n",
    "        \"tags\": [\"alpha\", \"beta\"] if i % 2 == 0 else [\"gamma\"],\n",
    "    }\n",
    "    for i in range(500)\n",
    "]\n",
    "\n",
    "# JSON\n",
    "t0 = time.perf_counter()\n",
    "json_bytes: bytes = json.dumps(test_data).encode(\"utf-8\")\n",
    "json_time = time.perf_counter() - t0\n",
    "\n",
    "# Pickle\n",
    "t0 = time.perf_counter()\n",
    "pickle_bytes: bytes = pickle.dumps(test_data, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "pickle_time = time.perf_counter() - t0\n",
    "\n",
    "# CSV (flat structure only)\n",
    "t0 = time.perf_counter()\n",
    "csv_buffer = StringIO()\n",
    "writer = csv.DictWriter(csv_buffer, fieldnames=[\"id\", \"name\", \"value\", \"active\"])\n",
    "writer.writeheader()\n",
    "for item in test_data:\n",
    "    writer.writerow({k: item[k] for k in [\"id\", \"name\", \"value\", \"active\"]})\n",
    "csv_bytes: bytes = csv_buffer.getvalue().encode(\"utf-8\")\n",
    "csv_time = time.perf_counter() - t0\n",
    "\n",
    "print(f\"Serialization comparison (500 records):\")\n",
    "print(f\"{'Format':<10s} {'Size (bytes)':>14s} {'Time (ms)':>12s} {'Human-readable':>16s} {'Cross-language':>16s}\")\n",
    "print(f\"{'-'*10} {'-'*14} {'-'*12} {'-'*16} {'-'*16}\")\n",
    "print(f\"{'JSON':<10s} {len(json_bytes):>14,} {json_time*1000:>11.2f}ms {'Yes':>16s} {'Yes':>16s}\")\n",
    "print(f\"{'CSV':<10s} {len(csv_bytes):>14,} {csv_time*1000:>11.2f}ms {'Yes':>16s} {'Yes':>16s}\")\n",
    "print(f\"{'Pickle':<10s} {len(pickle_bytes):>14,} {pickle_time*1000:>11.2f}ms {'No':>16s} {'No':>16s}\")\n",
    "\n",
    "print(f\"\\nKey trade-offs:\")\n",
    "print(f\"  JSON:   Universal, human-readable, but limited to basic types\")\n",
    "print(f\"  CSV:    Great for flat tabular data, widely supported by spreadsheets\")\n",
    "print(f\"  Pickle: Handles any Python object, but Python-only and insecure\")\n",
    "print(f\"  Struct: Compact binary, ideal for fixed-format records and protocols\")\n",
    "\n",
    "# Cleanup\n",
    "import shutil\n",
    "shutil.rmtree(work_dir)\n",
    "print(f\"\\nCleaned up {work_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### JSON (`json` module)\n",
    "- `dumps()`/`loads()` for strings, `dump()`/`load()` for files\n",
    "- Custom `JSONEncoder` and `object_hook` for non-standard types\n",
    "- Best for: APIs, config files, cross-language data exchange\n",
    "\n",
    "### CSV (`csv` module)\n",
    "- `reader`/`writer` for list-based rows\n",
    "- `DictReader`/`DictWriter` for named columns\n",
    "- Always use `newline=''` when opening CSV files\n",
    "- Best for: tabular data, spreadsheet interop, data science pipelines\n",
    "\n",
    "### Pickle (`pickle` module)\n",
    "- Serializes any Python object (including circular references)\n",
    "- **Never unpickle untrusted data** - arbitrary code execution risk\n",
    "- Use `protocol=pickle.HIGHEST_PROTOCOL` for best performance\n",
    "- Best for: caching, internal Python-to-Python data transfer\n",
    "\n",
    "### Struct (`struct` module)\n",
    "- Pack/unpack C-style binary data with format strings\n",
    "- Use `Struct` objects for repeated operations\n",
    "- Best for: binary file formats, network protocols, hardware interfaces\n",
    "\n",
    "### Choosing a Format\n",
    "1. **Cross-language exchange** -> JSON\n",
    "2. **Tabular data / spreadsheets** -> CSV\n",
    "3. **Complex Python objects (trusted source)** -> Pickle\n",
    "4. **Fixed binary format / protocols** -> Struct\n",
    "5. **Human-editable configuration** -> JSON or INI"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}