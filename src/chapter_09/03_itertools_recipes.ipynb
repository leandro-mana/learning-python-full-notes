{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Chapter 9: itertools and Functional Iteration Recipes\n",
    "\n",
    "**Chapter 9 - Learning Python, 5th Edition**\n",
    "\n",
    "The `itertools` module provides a collection of fast, memory-efficient tools\n",
    "for working with iterators. Combined with `functools.reduce` and generator\n",
    "pipelines, these tools enable powerful data processing without loading\n",
    "entire datasets into memory.\n",
    "\n",
    "## Key Concepts\n",
    "- **`itertools`**: Standard library of iterator building blocks\n",
    "- **Lazy evaluation**: Values computed on demand, not all at once\n",
    "- **Generator pipelines**: Chaining generators for multi-stage processing\n",
    "- **`functools.reduce`**: Cumulative reduction of an iterable to a single value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Section 1: Chaining Iterables\n",
    "\n",
    "`itertools.chain` concatenates multiple iterables into a single stream.\n",
    "`chain.from_iterable` does the same but accepts a single iterable of\n",
    "iterables (useful when you don't know the number of sources upfront)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# chain: concatenate known iterables\n",
    "frontend: list[str] = [\"HTML\", \"CSS\", \"JavaScript\"]\n",
    "backend: list[str] = [\"Python\", \"Go\", \"Rust\"]\n",
    "devops: list[str] = [\"Docker\", \"Kubernetes\"]\n",
    "\n",
    "all_skills: list[str] = list(itertools.chain(frontend, backend, devops))\n",
    "print(f\"All skills: {all_skills}\")\n",
    "\n",
    "# chain.from_iterable: flatten an iterable of iterables\n",
    "departments: list[list[str]] = [\n",
    "    [\"Alice\", \"Bob\"],\n",
    "    [\"Carol\", \"Dave\", \"Eve\"],\n",
    "    [\"Frank\"],\n",
    "]\n",
    "all_employees: list[str] = list(itertools.chain.from_iterable(departments))\n",
    "print(f\"All employees: {all_employees}\")\n",
    "\n",
    "# Practical: merge sorted sequences (preserving order within each)\n",
    "log_server_1: list[tuple[int, str]] = [(1, \"start\"), (3, \"request\"), (5, \"response\")]\n",
    "log_server_2: list[tuple[int, str]] = [(2, \"connect\"), (4, \"query\"), (6, \"disconnect\")]\n",
    "\n",
    "# chain then sort for a merged timeline\n",
    "merged_timeline = sorted(\n",
    "    itertools.chain(log_server_1, log_server_2),\n",
    "    key=lambda entry: entry[0]\n",
    ")\n",
    "print(f\"\\nMerged timeline:\")\n",
    "for ts, event in merged_timeline:\n",
    "    print(f\"  t={ts}: {event}\")\n",
    "\n",
    "# Use with heapq.merge for already-sorted sequences (more efficient)\n",
    "import heapq\n",
    "sorted_a: list[int] = [1, 4, 7, 10]\n",
    "sorted_b: list[int] = [2, 5, 8, 11]\n",
    "sorted_c: list[int] = [3, 6, 9, 12]\n",
    "merged = list(heapq.merge(sorted_a, sorted_b, sorted_c))\n",
    "print(f\"\\nheapq.merge: {merged}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Section 2: Lazy Slicing with `islice`\n",
    "\n",
    "`itertools.islice` provides slicing for any iterator -- including those\n",
    "that don't support indexing (generators, file objects, etc.). It consumes\n",
    "elements lazily without building an intermediate list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# You can't slice a generator with [start:stop]\n",
    "def infinite_counter(start: int = 0) -> itertools.count:\n",
    "    \"\"\"An infinite sequence of integers.\"\"\"\n",
    "    return itertools.count(start)\n",
    "\n",
    "# islice(iterable, stop)\n",
    "first_ten: list[int] = list(itertools.islice(infinite_counter(), 10))\n",
    "print(f\"First 10: {first_ten}\")\n",
    "\n",
    "# islice(iterable, start, stop)\n",
    "middle: list[int] = list(itertools.islice(infinite_counter(), 5, 15))\n",
    "print(f\"Elements 5-14: {middle}\")\n",
    "\n",
    "# islice(iterable, start, stop, step)\n",
    "every_third: list[int] = list(itertools.islice(infinite_counter(), 0, 30, 3))\n",
    "print(f\"Every 3rd (0-29): {every_third}\")\n",
    "\n",
    "# Practical: preview the first few lines of a large \"file\"\n",
    "import io\n",
    "\n",
    "large_file = io.StringIO(\"\\n\".join(f\"Line {i}: data_{i}\" for i in range(1, 10001)))\n",
    "\n",
    "print(\"\\nFirst 5 lines:\")\n",
    "for line in itertools.islice(large_file, 5):\n",
    "    print(f\"  {line.rstrip()}\")\n",
    "\n",
    "# Skip header (first line), take next 3 lines\n",
    "large_file.seek(0)\n",
    "print(\"\\nSkip header, next 3 lines:\")\n",
    "for line in itertools.islice(large_file, 1, 4):\n",
    "    print(f\"  {line.rstrip()}\")\n",
    "\n",
    "# Paginate results lazily\n",
    "def paginate(iterable, page_size: int, page_num: int) -> list:\n",
    "    \"\"\"Return a specific page of results from an iterable.\"\"\"\n",
    "    start = page_size * (page_num - 1)\n",
    "    return list(itertools.islice(iterable, start, start + page_size))\n",
    "\n",
    "all_items = range(1, 51)\n",
    "print(f\"\\nPage 1 (size=10): {paginate(iter(all_items), 10, 1)}\")\n",
    "print(f\"Page 3 (size=10): {paginate(iter(all_items), 10, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Section 3: Grouping with `groupby`\n",
    "\n",
    "`itertools.groupby` groups consecutive elements by a key function.\n",
    "The input **must be sorted** by the same key for correct grouping.\n",
    "It yields `(key, group_iterator)` pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from typing import NamedTuple\n",
    "\n",
    "\n",
    "class Sale(NamedTuple):\n",
    "    department: str\n",
    "    product: str\n",
    "    amount: float\n",
    "\n",
    "\n",
    "sales: list[Sale] = [\n",
    "    Sale(\"Electronics\", \"Laptop\", 999.99),\n",
    "    Sale(\"Electronics\", \"Phone\", 699.99),\n",
    "    Sale(\"Electronics\", \"Tablet\", 449.99),\n",
    "    Sale(\"Clothing\", \"Jacket\", 89.99),\n",
    "    Sale(\"Clothing\", \"Shoes\", 129.99),\n",
    "    Sale(\"Books\", \"Python 101\", 39.99),\n",
    "    Sale(\"Books\", \"Data Science\", 49.99),\n",
    "    Sale(\"Books\", \"Algorithms\", 59.99),\n",
    "]\n",
    "\n",
    "# Data must be sorted by the grouping key\n",
    "sales.sort(key=lambda s: s.department)\n",
    "\n",
    "print(\"Sales by department:\")\n",
    "for dept, group in itertools.groupby(sales, key=lambda s: s.department):\n",
    "    items = list(group)\n",
    "    total = sum(s.amount for s in items)\n",
    "    print(f\"  {dept}: {len(items)} items, total ${total:.2f}\")\n",
    "    for sale in items:\n",
    "        print(f\"    - {sale.product}: ${sale.amount:.2f}\")\n",
    "\n",
    "# Group numbers by parity\n",
    "numbers: list[int] = [1, 1, 2, 3, 3, 3, 4, 4, 5]\n",
    "print(f\"\\nRun-length encoding:\")\n",
    "for value, group in itertools.groupby(numbers):\n",
    "    count = sum(1 for _ in group)\n",
    "    print(f\"  {value} x {count}\")\n",
    "\n",
    "# Group by computed key: word length\n",
    "words: list[str] = sorted(\n",
    "    [\"cat\", \"dog\", \"fish\", \"bird\", \"ant\", \"bear\", \"wolf\", \"bee\", \"ox\"],\n",
    "    key=len\n",
    ")\n",
    "print(f\"\\nWords grouped by length:\")\n",
    "for length, group in itertools.groupby(words, key=len):\n",
    "    print(f\"  {length} letters: {list(group)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Section 4: Combinatorics\n",
    "\n",
    "`itertools.product`, `combinations`, and `permutations` generate\n",
    "combinatorial sequences lazily. These are essential for search problems,\n",
    "testing, and mathematical applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# product: Cartesian product (replaces nested for loops)\n",
    "colors: list[str] = [\"red\", \"blue\"]\n",
    "sizes: list[str] = [\"S\", \"M\", \"L\"]\n",
    "\n",
    "variants: list[tuple[str, str]] = list(itertools.product(colors, sizes))\n",
    "print(f\"Product variants ({len(variants)}):\")\n",
    "for color, size in variants:\n",
    "    print(f\"  {color}-{size}\")\n",
    "\n",
    "# product with repeat: all binary strings of length 3\n",
    "binary_3: list[tuple[int, ...]] = list(itertools.product([0, 1], repeat=3))\n",
    "print(f\"\\n3-bit binary: {binary_3}\")\n",
    "\n",
    "# combinations: unordered selections (no repetition)\n",
    "team: list[str] = [\"Alice\", \"Bob\", \"Carol\", \"Dave\"]\n",
    "pairs: list[tuple[str, str]] = list(itertools.combinations(team, 2))\n",
    "print(f\"\\nAll pairs from {team}:\")\n",
    "for pair in pairs:\n",
    "    print(f\"  {pair[0]} & {pair[1]}\")\n",
    "\n",
    "# combinations_with_replacement\n",
    "dice_doubles: list[tuple[int, int]] = list(\n",
    "    itertools.combinations_with_replacement(range(1, 7), 2)\n",
    ")\n",
    "print(f\"\\nDice pairs (with doubles): {len(dice_doubles)} combinations\")\n",
    "print(f\"First 5: {dice_doubles[:5]}\")\n",
    "\n",
    "# permutations: ordered arrangements\n",
    "letters: list[str] = [\"A\", \"B\", \"C\"]\n",
    "perms: list[tuple[str, ...]] = list(itertools.permutations(letters))\n",
    "print(f\"\\nPermutations of {letters}: {perms}\")\n",
    "\n",
    "# Partial permutations (pick 2 from 4)\n",
    "partial: list[tuple[str, ...]] = list(itertools.permutations(\"ABCD\", 2))\n",
    "print(f\"2-permutations of ABCD ({len(partial)}): {partial}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Section 5: `accumulate`, `starmap`, and `functools.reduce`\n",
    "\n",
    "`accumulate` produces running totals (or running applications of any\n",
    "binary function). `starmap` applies a function to pre-unpacked argument\n",
    "tuples. `reduce` collapses an iterable to a single value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import operator\n",
    "import functools\n",
    "\n",
    "# accumulate: running total\n",
    "monthly_revenue: list[int] = [100, 150, 120, 200, 180, 250]\n",
    "cumulative: list[int] = list(itertools.accumulate(monthly_revenue))\n",
    "print(f\"Monthly revenue: {monthly_revenue}\")\n",
    "print(f\"Cumulative:      {cumulative}\")\n",
    "\n",
    "# accumulate with custom function: running max\n",
    "data: list[int] = [3, 1, 4, 1, 5, 9, 2, 6, 5]\n",
    "running_max: list[int] = list(itertools.accumulate(data, max))\n",
    "print(f\"\\nData:        {data}\")\n",
    "print(f\"Running max: {running_max}\")\n",
    "\n",
    "# accumulate: factorial via running product\n",
    "factorials: list[int] = list(itertools.accumulate(range(1, 8), operator.mul))\n",
    "print(f\"\\nFactorials 1! to 7!: {factorials}\")\n",
    "\n",
    "# starmap: apply function to pre-unpacked argument tuples\n",
    "points: list[tuple[float, float, float, float]] = [\n",
    "    (0, 0, 3, 4),\n",
    "    (1, 1, 4, 5),\n",
    "    (0, 0, 0, 0),\n",
    "]\n",
    "\n",
    "import math\n",
    "\n",
    "def distance(x1: float, y1: float, x2: float, y2: float) -> float:\n",
    "    return math.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2)\n",
    "\n",
    "distances: list[float] = list(itertools.starmap(distance, points))\n",
    "print(f\"\\nPoint pairs -> distances: {[f'{d:.2f}' for d in distances]}\")\n",
    "\n",
    "# starmap with pow: [2^5, 3^2, 10^3]\n",
    "powers: list[int] = list(itertools.starmap(pow, [(2, 5), (3, 2), (10, 3)]))\n",
    "print(f\"Powers: {powers}\")\n",
    "\n",
    "# functools.reduce: collapse to single value\n",
    "numbers: list[int] = [1, 2, 3, 4, 5]\n",
    "\n",
    "product = functools.reduce(operator.mul, numbers)\n",
    "print(f\"\\nProduct of {numbers}: {product}\")\n",
    "\n",
    "# reduce with initial value: merge dicts left-to-right\n",
    "configs: list[dict[str, int]] = [\n",
    "    {\"timeout\": 30, \"retries\": 3},\n",
    "    {\"retries\": 5, \"port\": 8080},\n",
    "    {\"debug\": 1},\n",
    "]\n",
    "merged: dict[str, int] = functools.reduce(lambda a, b: {**a, **b}, configs)\n",
    "print(f\"Merged configs: {merged}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Section 6: Building Data Pipelines with Chained Generators\n",
    "\n",
    "Generators can be composed into multi-stage processing pipelines where\n",
    "each stage lazily transforms data from the previous stage. This pattern\n",
    "processes data element-by-element, keeping memory usage constant regardless\n",
    "of input size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator, Iterable\n",
    "\n",
    "\n",
    "# Pipeline stage 1: generate raw sensor readings\n",
    "def sensor_readings(n: int) -> Iterator[dict[str, float]]:\n",
    "    \"\"\"Simulate n sensor readings with occasional bad data.\"\"\"\n",
    "    import random\n",
    "    random.seed(42)\n",
    "    for i in range(n):\n",
    "        temp = random.gauss(25.0, 5.0)\n",
    "        # Occasionally inject invalid readings\n",
    "        if random.random() < 0.1:\n",
    "            temp = -999.0  # Sensor error\n",
    "        yield {\"id\": i, \"temp_c\": round(temp, 2)}\n",
    "\n",
    "\n",
    "# Pipeline stage 2: filter out bad readings\n",
    "def valid_only(readings: Iterable[dict[str, float]]) -> Iterator[dict[str, float]]:\n",
    "    \"\"\"Drop readings with invalid temperature.\"\"\"\n",
    "    for reading in readings:\n",
    "        if reading[\"temp_c\"] > -100:\n",
    "            yield reading\n",
    "\n",
    "\n",
    "# Pipeline stage 3: convert Celsius to Fahrenheit\n",
    "def add_fahrenheit(readings: Iterable[dict[str, float]]) -> Iterator[dict[str, float]]:\n",
    "    \"\"\"Add Fahrenheit conversion to each reading.\"\"\"\n",
    "    for reading in readings:\n",
    "        reading[\"temp_f\"] = round(reading[\"temp_c\"] * 9 / 5 + 32, 2)\n",
    "        yield reading\n",
    "\n",
    "\n",
    "# Pipeline stage 4: flag extreme values\n",
    "def flag_extremes(\n",
    "    readings: Iterable[dict[str, float]], threshold: float = 35.0\n",
    ") -> Iterator[dict]:\n",
    "    \"\"\"Flag readings above the threshold.\"\"\"\n",
    "    for reading in readings:\n",
    "        reading[\"extreme\"] = reading[\"temp_c\"] > threshold\n",
    "        yield reading\n",
    "\n",
    "\n",
    "# Compose the pipeline -- nothing executes yet\n",
    "raw = sensor_readings(20)\n",
    "cleaned = valid_only(raw)\n",
    "converted = add_fahrenheit(cleaned)\n",
    "flagged = flag_extremes(converted)\n",
    "\n",
    "# Drive the pipeline by consuming the final generator\n",
    "print(\"Sensor pipeline results:\")\n",
    "extreme_count = 0\n",
    "total_count = 0\n",
    "for reading in flagged:\n",
    "    total_count += 1\n",
    "    marker = \" [EXTREME]\" if reading[\"extreme\"] else \"\"\n",
    "    if reading[\"extreme\"]:\n",
    "        extreme_count += 1\n",
    "    print(f\"  #{reading['id']:02d}: {reading['temp_c']:6.2f}C / {reading['temp_f']:6.2f}F{marker}\")\n",
    "\n",
    "print(f\"\\nProcessed: {total_count} valid readings, {extreme_count} extreme\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Section 7: Real-World Example -- Processing a Large Log File Lazily\n",
    "\n",
    "This example demonstrates processing a large log file without loading it\n",
    "entirely into memory. Each stage of the pipeline handles one line at a time,\n",
    "so memory usage stays constant even for gigabyte-sized files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import re\n",
    "from typing import Iterator, Iterable, NamedTuple\n",
    "from collections import Counter\n",
    "import io\n",
    "\n",
    "\n",
    "class LogEntry(NamedTuple):\n",
    "    timestamp: str\n",
    "    level: str\n",
    "    message: str\n",
    "\n",
    "\n",
    "# Simulate a large log file\n",
    "SAMPLE_LOG = \"\"\"2024-01-15 08:00:01 INFO  Application started\n",
    "2024-01-15 08:00:02 DEBUG Loading configuration from /etc/app.conf\n",
    "2024-01-15 08:00:03 INFO  Connected to database\n",
    "2024-01-15 08:00:05 WARNING Slow query detected (2.3s)\n",
    "2024-01-15 08:00:10 ERROR Connection timeout to cache server\n",
    "2024-01-15 08:00:11 INFO  Retrying cache connection\n",
    "2024-01-15 08:00:12 INFO  Cache connection restored\n",
    "2024-01-15 08:00:15 DEBUG Processing batch of 1000 records\n",
    "2024-01-15 08:00:20 WARNING Memory usage at 85%\n",
    "2024-01-15 08:00:25 ERROR Failed to write to /var/log/audit.log: Permission denied\n",
    "2024-01-15 08:00:30 INFO  Batch processing complete\n",
    "2024-01-15 08:00:31 DEBUG Cleaning up temporary files\n",
    "2024-01-15 08:00:35 WARNING Disk usage at 90%\n",
    "2024-01-15 08:00:40 ERROR OutOfMemoryError in worker thread 3\n",
    "2024-01-15 08:00:41 INFO  Restarting worker thread 3\n",
    "2024-01-15 08:00:45 INFO  Health check passed\n",
    "\"\"\"\n",
    "\n",
    "LOG_PATTERN = re.compile(\n",
    "    r\"^(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})\\s+(\\w+)\\s+(.+)$\"\n",
    ")\n",
    "\n",
    "\n",
    "# Stage 1: Read lines lazily (simulates reading a real file)\n",
    "def read_lines(source: io.StringIO) -> Iterator[str]:\n",
    "    \"\"\"Yield stripped lines from a file-like object.\"\"\"\n",
    "    for line in source:\n",
    "        stripped = line.rstrip()\n",
    "        if stripped:\n",
    "            yield stripped\n",
    "\n",
    "\n",
    "# Stage 2: Parse each line into a structured LogEntry\n",
    "def parse_entries(lines: Iterable[str]) -> Iterator[LogEntry]:\n",
    "    \"\"\"Parse raw log lines into LogEntry objects.\"\"\"\n",
    "    for line in lines:\n",
    "        match = LOG_PATTERN.match(line)\n",
    "        if match:\n",
    "            yield LogEntry(\n",
    "                timestamp=match.group(1),\n",
    "                level=match.group(2),\n",
    "                message=match.group(3),\n",
    "            )\n",
    "\n",
    "\n",
    "# Stage 3: Filter by log level\n",
    "def filter_level(\n",
    "    entries: Iterable[LogEntry], min_level: str = \"WARNING\"\n",
    ") -> Iterator[LogEntry]:\n",
    "    \"\"\"Keep only entries at or above the specified severity.\"\"\"\n",
    "    levels = {\"DEBUG\": 0, \"INFO\": 1, \"WARNING\": 2, \"ERROR\": 3, \"CRITICAL\": 4}\n",
    "    threshold = levels.get(min_level, 0)\n",
    "    for entry in entries:\n",
    "        if levels.get(entry.level, 0) >= threshold:\n",
    "            yield entry\n",
    "\n",
    "\n",
    "# Compose and run the pipeline\n",
    "log_file = io.StringIO(SAMPLE_LOG)\n",
    "lines = read_lines(log_file)\n",
    "entries = parse_entries(lines)\n",
    "warnings_and_errors = filter_level(entries, \"WARNING\")\n",
    "\n",
    "print(\"=== Warnings and Errors ===\")\n",
    "for entry in warnings_and_errors:\n",
    "    print(f\"  [{entry.level:7s}] {entry.timestamp} - {entry.message}\")\n",
    "\n",
    "# Analytics pass: count entries by level\n",
    "log_file.seek(0)\n",
    "all_entries = list(parse_entries(read_lines(log_file)))\n",
    "level_counts = Counter(e.level for e in all_entries)\n",
    "\n",
    "print(f\"\\n=== Log Level Distribution ===\")\n",
    "for level, count in level_counts.most_common():\n",
    "    bar = \"#\" * (count * 3)\n",
    "    print(f\"  {level:8s} {bar} ({count})\")\n",
    "\n",
    "# Use islice for pagination: show only first 3 errors/warnings\n",
    "log_file.seek(0)\n",
    "first_3_issues = list(itertools.islice(\n",
    "    filter_level(parse_entries(read_lines(log_file)), \"WARNING\"),\n",
    "    3\n",
    "))\n",
    "print(f\"\\n=== First 3 Issues ===\")\n",
    "for entry in first_3_issues:\n",
    "    print(f\"  {entry.level}: {entry.message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### itertools Quick Reference\n",
    "| Function | Purpose |\n",
    "|---|---|\n",
    "| `chain(*iterables)` | Concatenate multiple iterables |\n",
    "| `chain.from_iterable(it)` | Flatten an iterable of iterables |\n",
    "| `islice(it, [start,] stop [, step])` | Lazy slicing of any iterator |\n",
    "| `groupby(it, key=)` | Group consecutive elements by key |\n",
    "| `product(*its, repeat=)` | Cartesian product |\n",
    "| `combinations(it, r)` | r-length unordered selections |\n",
    "| `permutations(it, r)` | r-length ordered arrangements |\n",
    "| `accumulate(it, func)` | Running totals / reductions |\n",
    "| `starmap(func, it)` | Apply function to unpacked tuples |\n",
    "\n",
    "### Key Patterns\n",
    "- **Always sort before `groupby`** -- it only groups consecutive elements\n",
    "- **Use `islice`** instead of `list(gen)[:n]` to avoid materializing the full sequence\n",
    "- **Chain generators** into pipelines for memory-efficient processing\n",
    "- **`functools.reduce`** for cumulative operations that collapse to a single value\n",
    "- **Generator pipelines** process data element-by-element, keeping memory constant"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}