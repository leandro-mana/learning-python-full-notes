{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# Chapter 36: Pytest Features\n",
    "\n",
    "This notebook covers the essential features of pytest, Python's most popular testing framework. Since pytest fixtures, parametrize decorators, and markers require the pytest runner to execute, we demonstrate the concepts by showing the code patterns, explaining how they work, and using regular function calls to simulate behavior.\n",
    "\n",
    "## Key Concepts\n",
    "- **Fixtures**: Reusable setup/teardown functions that provide test data and resources\n",
    "- **Parametrize**: Run the same test logic with multiple sets of inputs\n",
    "- **Markers**: Tag tests for selective execution (skip, xfail, custom)\n",
    "- **monkeypatch**: Temporarily modify objects, dictionaries, and environment variables\n",
    "- **tmp_path**: Get a unique temporary directory for each test\n",
    "- **pytest.raises**: Assert that code raises a specific exception\n",
    "- **pytest.approx**: Compare floating-point numbers with tolerance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c2d3e4",
   "metadata": {},
   "source": [
    "## Section 1: Understanding Fixtures\n",
    "\n",
    "Fixtures are functions decorated with `@pytest.fixture` that provide reusable test data or resources. Tests declare dependencies on fixtures by accepting them as parameters. Here we simulate fixture behavior with regular functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d2e3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In a real pytest file, you'd write:\n",
    "#\n",
    "# @pytest.fixture\n",
    "# def sample_user() -> dict[str, str | int]:\n",
    "#     return {\"name\": \"Alice\", \"age\": 30}\n",
    "#\n",
    "# def test_user_name(sample_user: dict[str, str | int]) -> None:\n",
    "#     assert sample_user[\"name\"] == \"Alice\"\n",
    "\n",
    "# Simulating fixture behavior with a regular function\n",
    "def sample_user_fixture() -> dict[str, str | int]:\n",
    "    \"\"\"Fixture that provides a sample user dictionary.\"\"\"\n",
    "    return {\"name\": \"Alice\", \"age\": 30}\n",
    "\n",
    "\n",
    "# Simulate a test that uses the fixture\n",
    "def test_user_name() -> None:\n",
    "    user: dict[str, str | int] = sample_user_fixture()\n",
    "    assert user[\"name\"] == \"Alice\"\n",
    "    assert isinstance(user[\"age\"], int)\n",
    "    print(f\"User: {user}\")\n",
    "    print(f\"Name check passed: {user['name']} == 'Alice'\")\n",
    "    print(f\"Age is int: {isinstance(user['age'], int)}\")\n",
    "\n",
    "\n",
    "test_user_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e2f3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixtures can have different scopes:\n",
    "# - \"function\" (default): created fresh for each test\n",
    "# - \"class\": shared across all tests in a class\n",
    "# - \"module\": shared across all tests in a module\n",
    "# - \"session\": shared across the entire test session\n",
    "\n",
    "# Simulating a fixture with setup and teardown using yield\n",
    "from typing import Generator\n",
    "\n",
    "\n",
    "def database_connection_fixture() -> Generator[dict[str, str], None, None]:\n",
    "    \"\"\"Fixture with setup and teardown (yield fixture).\"\"\"\n",
    "    # Setup phase\n",
    "    connection: dict[str, str] = {\"host\": \"localhost\", \"status\": \"connected\"}\n",
    "    print(f\"SETUP: Opening connection to {connection['host']}\")\n",
    "\n",
    "    yield connection  # This is what the test receives\n",
    "\n",
    "    # Teardown phase (runs after test completes)\n",
    "    connection[\"status\"] = \"closed\"\n",
    "    print(f\"TEARDOWN: Connection closed\")\n",
    "\n",
    "\n",
    "# Simulate the full fixture lifecycle\n",
    "gen: Generator[dict[str, str], None, None] = database_connection_fixture()\n",
    "conn: dict[str, str] = next(gen)  # Setup runs, we get the yielded value\n",
    "print(f\"TEST: Using connection with status '{conn['status']}'\")\n",
    "try:\n",
    "    next(gen)  # Trigger teardown\n",
    "except StopIteration:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f2a3b4",
   "metadata": {},
   "source": [
    "## Section 2: Parametrize -- Multiple Inputs, One Test\n",
    "\n",
    "The `@pytest.mark.parametrize` decorator runs a single test function with multiple sets of arguments. This eliminates repetitive test code. We simulate this pattern with a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a2b3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In a real pytest file:\n",
    "#\n",
    "# @pytest.mark.parametrize(\"value, expected\", [\n",
    "#     (1, 1), (2, 4), (3, 9), (4, 16), (0, 0),\n",
    "# ])\n",
    "# def test_square(value: int, expected: int) -> None:\n",
    "#     assert value ** 2 == expected\n",
    "\n",
    "# Simulating parametrize with a loop\n",
    "test_cases: list[tuple[int, int]] = [\n",
    "    (1, 1),\n",
    "    (2, 4),\n",
    "    (3, 9),\n",
    "    (4, 16),\n",
    "    (0, 0),\n",
    "]\n",
    "\n",
    "print(\"Parametrized square tests:\")\n",
    "for value, expected in test_cases:\n",
    "    result: int = value ** 2\n",
    "    passed: bool = result == expected\n",
    "    status: str = \"PASSED\" if passed else \"FAILED\"\n",
    "    print(f\"  {value}^2 = {result}, expected {expected} -> {status}\")\n",
    "    assert result == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b3c4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametrize with multiple parameters and IDs\n",
    "# In pytest:\n",
    "#\n",
    "# @pytest.mark.parametrize(\"text, expected_words\", [\n",
    "#     (\"hello world\", 2),\n",
    "#     (\"\", 0),\n",
    "#     (\"one\", 1),\n",
    "#     (\"  spaces  between  \", 2),\n",
    "# ], ids=[\"two_words\", \"empty\", \"single\", \"extra_spaces\"])\n",
    "\n",
    "def count_words(text: str) -> int:\n",
    "    \"\"\"Count non-empty words in a string.\"\"\"\n",
    "    return len(text.split()) if text.strip() else 0\n",
    "\n",
    "\n",
    "word_count_cases: list[tuple[str, int, str]] = [\n",
    "    (\"hello world\", 2, \"two_words\"),\n",
    "    (\"\", 0, \"empty\"),\n",
    "    (\"one\", 1, \"single\"),\n",
    "    (\"  spaces  between  \", 2, \"extra_spaces\"),\n",
    "]\n",
    "\n",
    "print(\"Parametrized word count tests:\")\n",
    "for text, expected_words, test_id in word_count_cases:\n",
    "    result: int = count_words(text)\n",
    "    passed: bool = result == expected_words\n",
    "    status: str = \"PASSED\" if passed else \"FAILED\"\n",
    "    print(f\"  [{test_id}] count_words({text!r}) = {result}, expected {expected_words} -> {status}\")\n",
    "    assert result == expected_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5",
   "metadata": {},
   "source": [
    "## Section 3: Markers -- Tagging and Controlling Tests\n",
    "\n",
    "Markers let you tag tests for selective execution. Common built-in markers include `skip`, `skipif`, and `xfail`. Custom markers let you group tests by category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d3e4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# In pytest:\n",
    "#\n",
    "# @pytest.mark.skip(reason=\"Not implemented yet\")\n",
    "# def test_future_feature() -> None: ...\n",
    "#\n",
    "# @pytest.mark.skipif(sys.platform == \"win32\", reason=\"Unix only\")\n",
    "# def test_unix_feature() -> None: ...\n",
    "#\n",
    "# @pytest.mark.xfail(reason=\"Known bug #123\")\n",
    "# def test_known_bug() -> None: ...\n",
    "\n",
    "# Simulating marker behavior\n",
    "def simulate_skip(reason: str) -> None:\n",
    "    print(f\"  SKIPPED: {reason}\")\n",
    "\n",
    "\n",
    "def simulate_skipif(condition: bool, reason: str) -> bool:\n",
    "    if condition:\n",
    "        print(f\"  SKIPPED: {reason}\")\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def simulate_xfail(test_func: object, reason: str) -> None:\n",
    "    try:\n",
    "        if callable(test_func):\n",
    "            test_func()\n",
    "        print(f\"  XPASS (unexpectedly passed): {reason}\")\n",
    "    except AssertionError:\n",
    "        print(f\"  XFAIL (expected failure): {reason}\")\n",
    "\n",
    "\n",
    "print(\"Marker demonstrations:\")\n",
    "print(\"\\n@pytest.mark.skip:\")\n",
    "simulate_skip(\"Not implemented yet\")\n",
    "\n",
    "print(\"\\n@pytest.mark.skipif:\")\n",
    "simulate_skipif(sys.platform != \"win32\", \"Windows only test\")\n",
    "\n",
    "print(\"\\n@pytest.mark.xfail:\")\n",
    "simulate_xfail(lambda: None, \"Known bug #123 -- test passed unexpectedly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e3f4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom markers allow grouping tests by category\n",
    "# In pytest.ini or pyproject.toml:\n",
    "#   [tool.pytest.ini_options]\n",
    "#   markers = [\n",
    "#       \"slow: marks tests as slow\",\n",
    "#       \"integration: marks integration tests\",\n",
    "#   ]\n",
    "#\n",
    "# Usage:\n",
    "#   @pytest.mark.slow\n",
    "#   def test_large_dataset() -> None: ...\n",
    "#\n",
    "# Run only slow tests:  pytest -m slow\n",
    "# Exclude slow tests:   pytest -m \"not slow\"\n",
    "\n",
    "# Simulating custom markers with a registry\n",
    "marker_registry: dict[str, list[str]] = {\n",
    "    \"slow\": [],\n",
    "    \"integration\": [],\n",
    "    \"unit\": [],\n",
    "}\n",
    "\n",
    "\n",
    "def register_test(marker: str, test_name: str) -> None:\n",
    "    marker_registry[marker].append(test_name)\n",
    "\n",
    "\n",
    "register_test(\"unit\", \"test_add\")\n",
    "register_test(\"unit\", \"test_subtract\")\n",
    "register_test(\"integration\", \"test_database_query\")\n",
    "register_test(\"slow\", \"test_large_dataset\")\n",
    "register_test(\"slow\", \"test_performance_benchmark\")\n",
    "\n",
    "print(\"Custom marker registry:\")\n",
    "for marker, tests in marker_registry.items():\n",
    "    print(f\"  @pytest.mark.{marker}: {tests}\")\n",
    "\n",
    "# Filter by marker (simulating pytest -m)\n",
    "selected_marker: str = \"unit\"\n",
    "print(f\"\\nRunning with -m {selected_marker}: {marker_registry[selected_marker]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f3a4b5",
   "metadata": {},
   "source": [
    "## Section 4: Monkeypatch -- Temporary Modifications\n",
    "\n",
    "The `monkeypatch` fixture lets you temporarily modify attributes, dictionary items, and environment variables during a test. Changes are automatically reverted after the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a3b4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In pytest, monkeypatch is a built-in fixture:\n",
    "#\n",
    "# def test_config(monkeypatch) -> None:\n",
    "#     monkeypatch.setattr(Config, \"debug\", True)\n",
    "#     assert Config.debug is True\n",
    "\n",
    "# Simulating monkeypatch.setattr\n",
    "class Config:\n",
    "    debug: bool = False\n",
    "    log_level: str = \"INFO\"\n",
    "\n",
    "\n",
    "print(f\"Before monkeypatch: Config.debug = {Config.debug}\")\n",
    "print(f\"Before monkeypatch: Config.log_level = {Config.log_level}\")\n",
    "\n",
    "# Save originals, patch, then restore (what monkeypatch does internally)\n",
    "original_debug: bool = Config.debug\n",
    "original_level: str = Config.log_level\n",
    "\n",
    "Config.debug = True\n",
    "Config.log_level = \"DEBUG\"\n",
    "\n",
    "print(f\"\\nDuring test: Config.debug = {Config.debug}\")\n",
    "print(f\"During test: Config.log_level = {Config.log_level}\")\n",
    "assert Config.debug is True\n",
    "assert Config.log_level == \"DEBUG\"\n",
    "\n",
    "# Restore (monkeypatch does this automatically)\n",
    "Config.debug = original_debug\n",
    "Config.log_level = original_level\n",
    "\n",
    "print(f\"\\nAfter test: Config.debug = {Config.debug}\")\n",
    "print(f\"After test: Config.log_level = {Config.log_level}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b4c5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# monkeypatch.setenv / monkeypatch.delenv for environment variables\n",
    "# In pytest:\n",
    "#\n",
    "# def test_env(monkeypatch) -> None:\n",
    "#     monkeypatch.setenv(\"DATABASE_URL\", \"sqlite:///:memory:\")\n",
    "#     assert os.environ[\"DATABASE_URL\"] == \"sqlite:///:memory:\"\n",
    "\n",
    "# Simulating monkeypatch.setenv\n",
    "env_key: str = \"MY_TEST_VAR\"\n",
    "print(f\"Before: {env_key!r} in os.environ = {env_key in os.environ}\")\n",
    "\n",
    "os.environ[env_key] = \"test_value\"\n",
    "print(f\"During test: os.environ[{env_key!r}] = {os.environ[env_key]!r}\")\n",
    "assert os.environ[env_key] == \"test_value\"\n",
    "\n",
    "# Cleanup (monkeypatch handles this automatically)\n",
    "del os.environ[env_key]\n",
    "print(f\"After test: {env_key!r} in os.environ = {env_key in os.environ}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c4d5e6",
   "metadata": {},
   "source": [
    "## Section 5: tmp_path -- Temporary Directories\n",
    "\n",
    "The `tmp_path` fixture provides a `pathlib.Path` to a unique temporary directory for each test. Files created there are automatically cleaned up. This is ideal for testing file I/O."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d4e5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import tempfile\n",
    "\n",
    "# In pytest:\n",
    "#\n",
    "# def test_write_file(tmp_path: Path) -> None:\n",
    "#     file = tmp_path / \"test.txt\"\n",
    "#     file.write_text(\"hello\")\n",
    "#     assert file.read_text() == \"hello\"\n",
    "\n",
    "# Simulating tmp_path with tempfile\n",
    "with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "    tmp_path: Path = Path(tmp_dir)\n",
    "\n",
    "    # Write a file\n",
    "    test_file: Path = tmp_path / \"test.txt\"\n",
    "    test_file.write_text(\"hello\")\n",
    "\n",
    "    # Read it back\n",
    "    content: str = test_file.read_text()\n",
    "    print(f\"File path: {test_file}\")\n",
    "    print(f\"File exists: {test_file.exists()}\")\n",
    "    print(f\"Content: {content!r}\")\n",
    "    assert content == \"hello\"\n",
    "\n",
    "    # Create subdirectories\n",
    "    sub_dir: Path = tmp_path / \"subdir\"\n",
    "    sub_dir.mkdir()\n",
    "    nested_file: Path = sub_dir / \"data.json\"\n",
    "    nested_file.write_text('{\"key\": \"value\"}')\n",
    "\n",
    "    print(f\"\\nNested file: {nested_file}\")\n",
    "    print(f\"Nested content: {nested_file.read_text()}\")\n",
    "\n",
    "# After the context manager, everything is cleaned up\n",
    "print(f\"\\nTemp dir still exists: {Path(tmp_dir).exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e4f5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Practical example: testing a function that reads/writes files\n",
    "def save_report(directory: Path, name: str, lines: list[str]) -> Path:\n",
    "    \"\"\"Save a report file and return the path.\"\"\"\n",
    "    file_path: Path = directory / f\"{name}.txt\"\n",
    "    file_path.write_text(\"\\n\".join(lines))\n",
    "    return file_path\n",
    "\n",
    "\n",
    "def load_report(file_path: Path) -> list[str]:\n",
    "    \"\"\"Load a report file and return lines.\"\"\"\n",
    "    return file_path.read_text().splitlines()\n",
    "\n",
    "\n",
    "# Test using a temporary directory (simulating tmp_path)\n",
    "with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "    tmp_path: Path = Path(tmp_dir)\n",
    "\n",
    "    report_lines: list[str] = [\"Header\", \"Line 1\", \"Line 2\"]\n",
    "    saved_path: Path = save_report(tmp_path, \"monthly\", report_lines)\n",
    "\n",
    "    print(f\"Saved to: {saved_path.name}\")\n",
    "    print(f\"File exists: {saved_path.exists()}\")\n",
    "\n",
    "    loaded: list[str] = load_report(saved_path)\n",
    "    print(f\"Loaded lines: {loaded}\")\n",
    "    assert loaded == report_lines\n",
    "    print(\"Round-trip test PASSED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f4a5b6",
   "metadata": {},
   "source": [
    "## Section 6: pytest.raises -- Testing Exceptions\n",
    "\n",
    "`pytest.raises` is a context manager that asserts a block of code raises a specific exception. It can also match against the exception message using a regex pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a4b5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "# In pytest:\n",
    "#\n",
    "# def test_raises() -> None:\n",
    "#     with pytest.raises(ValueError, match=\"invalid\"):\n",
    "#         raise ValueError(\"invalid literal\")\n",
    "\n",
    "# Simulating pytest.raises behavior\n",
    "def assert_raises(\n",
    "    exception_type: type[BaseException],\n",
    "    callable_fn: object,\n",
    "    match: str | None = None,\n",
    ") -> None:\n",
    "    \"\"\"Simulate pytest.raises with optional message matching.\"\"\"\n",
    "    try:\n",
    "        if callable(callable_fn):\n",
    "            callable_fn()\n",
    "        print(f\"  FAILED: {exception_type.__name__} was not raised\")\n",
    "    except exception_type as e:\n",
    "        if match and not re.search(match, str(e)):\n",
    "            print(f\"  FAILED: Message {str(e)!r} did not match {match!r}\")\n",
    "        else:\n",
    "            print(f\"  PASSED: Caught {exception_type.__name__}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  FAILED: Expected {exception_type.__name__}, got {type(e).__name__}: {e}\")\n",
    "\n",
    "\n",
    "# Test 1: Basic exception check\n",
    "print(\"Test: ValueError is raised\")\n",
    "assert_raises(ValueError, lambda: int(\"not_a_number\"))\n",
    "\n",
    "# Test 2: Exception with message matching\n",
    "print(\"\\nTest: ValueError with 'invalid' in message\")\n",
    "assert_raises(\n",
    "    ValueError,\n",
    "    lambda: (_ for _ in ()).throw(ValueError(\"invalid literal\")),\n",
    "    match=\"invalid\",\n",
    ")\n",
    "\n",
    "# Test 3: ZeroDivisionError\n",
    "print(\"\\nTest: ZeroDivisionError on division by zero\")\n",
    "assert_raises(ZeroDivisionError, lambda: 1 / 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b5c6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using actual pytest.raises (it works in notebooks too)\n",
    "import pytest\n",
    "\n",
    "\n",
    "def validate_age(age: int) -> int:\n",
    "    \"\"\"Validate that age is a positive integer.\"\"\"\n",
    "    if age < 0:\n",
    "        raise ValueError(f\"Age cannot be negative: {age}\")\n",
    "    if age > 150:\n",
    "        raise ValueError(f\"Age is unrealistically high: {age}\")\n",
    "    return age\n",
    "\n",
    "\n",
    "# Test valid input\n",
    "result: int = validate_age(25)\n",
    "print(f\"validate_age(25) = {result}\")\n",
    "\n",
    "# Test negative age\n",
    "with pytest.raises(ValueError, match=\"cannot be negative\"):\n",
    "    validate_age(-5)\n",
    "print(\"Caught ValueError for negative age\")\n",
    "\n",
    "# Test too-high age\n",
    "with pytest.raises(ValueError, match=\"unrealistically high\"):\n",
    "    validate_age(200)\n",
    "print(\"Caught ValueError for too-high age\")\n",
    "\n",
    "# Access the exception info\n",
    "with pytest.raises(ValueError) as exc_info:\n",
    "    validate_age(-1)\n",
    "print(f\"\\nException type: {type(exc_info.value).__name__}\")\n",
    "print(f\"Exception message: {exc_info.value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c5d6e7",
   "metadata": {},
   "source": [
    "## Section 7: pytest.approx -- Floating-Point Comparison\n",
    "\n",
    "Floating-point arithmetic can produce tiny rounding errors. `pytest.approx` compares values with a configurable tolerance, avoiding brittle equality checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d5e6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "# The classic floating-point problem\n",
    "result: float = 0.1 + 0.2\n",
    "print(f\"0.1 + 0.2 = {result}\")\n",
    "print(f\"0.1 + 0.2 == 0.3: {result == 0.3}\")\n",
    "\n",
    "# pytest.approx handles this gracefully\n",
    "print(f\"0.1 + 0.2 == pytest.approx(0.3): {result == pytest.approx(0.3)}\")\n",
    "assert 0.1 + 0.2 == pytest.approx(0.3)\n",
    "\n",
    "# Works with lists too\n",
    "computed: list[float] = [0.1, 0.2, 0.1 + 0.2]\n",
    "expected: list[float] = [0.1, 0.2, 0.3]\n",
    "print(f\"\\nList comparison: {computed == pytest.approx(expected)}\")\n",
    "assert computed == pytest.approx(expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e5f6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import math\n",
    "\n",
    "# Custom tolerance with abs and rel parameters\n",
    "# Default: rel=1e-6, abs=1e-12\n",
    "\n",
    "# Absolute tolerance: values must be within abs of each other\n",
    "assert 100.0 == pytest.approx(100.001, abs=0.01)\n",
    "print(f\"100.0 ~= 100.001 (abs=0.01): True\")\n",
    "\n",
    "# Relative tolerance: values must be within rel fraction of expected\n",
    "assert 100.0 == pytest.approx(100.05, rel=0.001)\n",
    "print(f\"100.0 ~= 100.05 (rel=0.001): True\")\n",
    "\n",
    "# Practical example: testing math functions\n",
    "angle: float = math.pi / 4\n",
    "sin_val: float = math.sin(angle)\n",
    "cos_val: float = math.cos(angle)\n",
    "expected_val: float = math.sqrt(2) / 2\n",
    "\n",
    "assert sin_val == pytest.approx(expected_val)\n",
    "assert cos_val == pytest.approx(expected_val)\n",
    "print(f\"\\nsin(pi/4) = {sin_val}\")\n",
    "print(f\"cos(pi/4) = {cos_val}\")\n",
    "print(f\"sqrt(2)/2 = {expected_val}\")\n",
    "print(f\"All approximately equal: True\")\n",
    "\n",
    "# Dictionary values with approx\n",
    "computed_stats: dict[str, float] = {\"mean\": 3.33333, \"std\": 1.41421}\n",
    "expected_stats: dict[str, float] = {\"mean\": 10 / 3, \"std\": math.sqrt(2)}\n",
    "assert computed_stats == pytest.approx(expected_stats, abs=1e-4)\n",
    "print(f\"\\nDict approx comparison passed with abs=1e-4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f5a6b7",
   "metadata": {},
   "source": [
    "## Section 8: Putting It All Together\n",
    "\n",
    "Here is a complete example showing how multiple pytest features would work together in a real test file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a5b6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Module under test\n",
    "class UserService:\n",
    "    \"\"\"Simple user service for demonstration.\"\"\"\n",
    "\n",
    "    def __init__(self, data_dir: Path) -> None:\n",
    "        self.data_dir: Path = data_dir\n",
    "\n",
    "    def save_user(self, name: str, email: str) -> Path:\n",
    "        \"\"\"Save user data to a file.\"\"\"\n",
    "        if not name:\n",
    "            raise ValueError(\"Name cannot be empty\")\n",
    "        file_path: Path = self.data_dir / f\"{name.lower()}.txt\"\n",
    "        file_path.write_text(f\"{name}\\n{email}\")\n",
    "        return file_path\n",
    "\n",
    "    def load_user(self, name: str) -> dict[str, str]:\n",
    "        \"\"\"Load user data from a file.\"\"\"\n",
    "        file_path: Path = self.data_dir / f\"{name.lower()}.txt\"\n",
    "        if not file_path.exists():\n",
    "            raise FileNotFoundError(f\"User {name!r} not found\")\n",
    "        lines: list[str] = file_path.read_text().splitlines()\n",
    "        return {\"name\": lines[0], \"email\": lines[1]}\n",
    "\n",
    "\n",
    "# --- Test suite (simulating pytest features) ---\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "    tmp_path: Path = Path(tmp_dir)\n",
    "    service: UserService = UserService(tmp_path)\n",
    "\n",
    "    # Test 1: Fixture-provided service + tmp_path for file I/O\n",
    "    saved: Path = service.save_user(\"Alice\", \"alice@example.com\")\n",
    "    assert saved.exists()\n",
    "    print(\"test_save_user: PASSED\")\n",
    "\n",
    "    # Test 2: Round-trip save/load\n",
    "    user: dict[str, str] = service.load_user(\"Alice\")\n",
    "    assert user == {\"name\": \"Alice\", \"email\": \"alice@example.com\"}\n",
    "    print(\"test_load_user: PASSED\")\n",
    "\n",
    "    # Test 3: pytest.raises for empty name\n",
    "    try:\n",
    "        service.save_user(\"\", \"no@email.com\")\n",
    "        print(\"test_empty_name: FAILED (no exception)\")\n",
    "    except ValueError as e:\n",
    "        assert \"cannot be empty\" in str(e)\n",
    "        print(\"test_empty_name_raises: PASSED\")\n",
    "\n",
    "    # Test 4: pytest.raises for missing user\n",
    "    try:\n",
    "        service.load_user(\"nonexistent\")\n",
    "        print(\"test_missing_user: FAILED (no exception)\")\n",
    "    except FileNotFoundError as e:\n",
    "        assert \"not found\" in str(e)\n",
    "        print(\"test_missing_user_raises: PASSED\")\n",
    "\n",
    "    # Test 5: Parametrize-style multiple users\n",
    "    users: list[tuple[str, str]] = [\n",
    "        (\"Bob\", \"bob@test.com\"),\n",
    "        (\"Carol\", \"carol@test.com\"),\n",
    "        (\"Dave\", \"dave@test.com\"),\n",
    "    ]\n",
    "    for name, email in users:\n",
    "        service.save_user(name, email)\n",
    "        loaded: dict[str, str] = service.load_user(name)\n",
    "        assert loaded[\"name\"] == name\n",
    "        assert loaded[\"email\"] == email\n",
    "    print(f\"test_parametrized_users ({len(users)} cases): PASSED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b6c7d8",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Fixtures\n",
    "- **`@pytest.fixture`**: Declare reusable setup functions; tests request them by parameter name\n",
    "- **Yield fixtures**: Use `yield` to provide a value and run teardown code after the test\n",
    "- **Scopes**: `function` (default), `class`, `module`, `session` control fixture lifetime\n",
    "\n",
    "### Parametrize\n",
    "- **`@pytest.mark.parametrize(\"args\", [...])`**: Run one test with many input sets\n",
    "- **`ids` parameter**: Give human-readable names to each test case\n",
    "\n",
    "### Markers\n",
    "- **`@pytest.mark.skip`**: Unconditionally skip a test\n",
    "- **`@pytest.mark.skipif(condition)`**: Skip based on a condition (platform, version, etc.)\n",
    "- **`@pytest.mark.xfail`**: Mark a test as expected to fail\n",
    "- **Custom markers**: Tag tests for selective execution with `-m`\n",
    "\n",
    "### Built-in Fixtures\n",
    "- **`monkeypatch`**: Temporarily modify attributes (`setattr`), env vars (`setenv`), and dict items (`setitem`)\n",
    "- **`tmp_path`**: Provides a unique `pathlib.Path` temporary directory per test\n",
    "\n",
    "### Assertion Helpers\n",
    "- **`pytest.raises(ExcType, match=pattern)`**: Assert code raises a specific exception with optional message matching\n",
    "- **`pytest.approx(value, rel=1e-6, abs=1e-12)`**: Floating-point comparison with tolerance; works with scalars, lists, and dicts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}