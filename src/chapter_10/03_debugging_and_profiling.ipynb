{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Debugging and Profiling\n",
    "\n",
    "**Chapter 10 - Learning Python, 5th Edition**\n",
    "\n",
    "Effective debugging and performance analysis are essential professional skills.\n",
    "Python provides built-in tools ranging from simple `print` debugging to the\n",
    "`pdb` debugger, `timeit` for micro-benchmarks, and `cProfile` for profiling.\n",
    "Understanding common performance patterns helps you write efficient code\n",
    "from the start."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Print Debugging and the logging Module\n",
    "\n",
    "While `print()` is the simplest debugging tool, the `logging` module provides\n",
    "configurable, leveled output that can be left in production code. Logging levels\n",
    "(DEBUG, INFO, WARNING, ERROR, CRITICAL) let you control verbosity without\n",
    "changing code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "\n",
    "# --- Print debugging: useful but limited ---\n",
    "def find_first_duplicate(items: list[int]) -> int | None:\n",
    "    \"\"\"Find the first duplicate in a list (with print debugging).\"\"\"\n",
    "    seen: set[int] = set()\n",
    "    for i, item in enumerate(items):\n",
    "        print(f\"  [DEBUG] i={i}, item={item}, seen={seen}\")  # Debug output\n",
    "        if item in seen:\n",
    "            print(f\"  [DEBUG] Found duplicate: {item} at index {i}\")\n",
    "            return item\n",
    "        seen.add(item)\n",
    "    return None\n",
    "\n",
    "\n",
    "print(\"=== Print debugging ===\")\n",
    "result = find_first_duplicate([3, 1, 4, 1, 5])\n",
    "print(f\"  Result: {result}\")\n",
    "\n",
    "\n",
    "# --- Logging: professional, configurable debugging ---\n",
    "# Reset any existing handlers for clean demo output\n",
    "root_logger = logging.getLogger()\n",
    "for handler in root_logger.handlers[:]:\n",
    "    root_logger.removeHandler(handler)\n",
    "\n",
    "# Configure logging with a structured format\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(asctime)s [%(levelname)-8s] %(name)s: %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\",\n",
    "    stream=sys.stdout,\n",
    ")\n",
    "\n",
    "# Create a module-specific logger\n",
    "logger = logging.getLogger(\"myapp.processor\")\n",
    "\n",
    "\n",
    "def process_batch(items: list[str]) -> list[str]:\n",
    "    \"\"\"Process a batch of items with proper logging.\"\"\"\n",
    "    logger.info(\"Starting batch processing of %d items\", len(items))\n",
    "    results: list[str] = []\n",
    "\n",
    "    for i, item in enumerate(items):\n",
    "        logger.debug(\"Processing item %d: %r\", i, item)\n",
    "        if not item.strip():\n",
    "            logger.warning(\"Skipping empty item at index %d\", i)\n",
    "            continue\n",
    "        try:\n",
    "            processed = item.strip().upper()\n",
    "            results.append(processed)\n",
    "        except Exception as e:\n",
    "            logger.error(\"Failed to process item %d: %s\", i, e, exc_info=True)\n",
    "\n",
    "    logger.info(\"Batch complete: %d/%d items processed\", len(results), len(items))\n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"\\n=== Logging module ===\")\n",
    "results = process_batch([\"  hello \", \"\", \"world  \", \"  python\"])\n",
    "print(f\"\\n  Results: {results}\")\n",
    "\n",
    "# Changing log level filters output without code changes\n",
    "print(\"\\n=== With WARNING level (less verbose) ===\")\n",
    "logger.setLevel(logging.WARNING)\n",
    "results = process_batch([\"test\", \"\", \"data\"])\n",
    "print(f\"  Results: {results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## The pdb Debugger\n",
    "\n",
    "Python's built-in debugger `pdb` lets you pause execution and inspect state\n",
    "interactively. Since Python 3.7, `breakpoint()` is the preferred way to\n",
    "enter the debugger.\n",
    "\n",
    "Essential pdb commands:\n",
    "\n",
    "| Command | Shortcut | Description |\n",
    "|---|---|---|\n",
    "| `next` | `n` | Execute next line (step OVER functions) |\n",
    "| `step` | `s` | Step INTO function calls |\n",
    "| `continue` | `c` | Continue until next breakpoint |\n",
    "| `print expr` | `p expr` | Print the value of an expression |\n",
    "| `list` | `l` | Show source code around current line |\n",
    "| `where` | `w` | Show the call stack (traceback) |\n",
    "| `up` / `down` | `u` / `d` | Move up/down the call stack |\n",
    "| `break` | `b` | Set a breakpoint |\n",
    "| `quit` | `q` | Quit the debugger |\n",
    "\n",
    "**Note**: pdb is interactive and does not work well in notebooks. The code\n",
    "below shows how you would use it in a normal Python script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdb usage examples (shown as reference; pdb is interactive, not for notebooks)\n",
    "\n",
    "pdb_example_basic = '''\n",
    "# === Method 1: breakpoint() (Python 3.7+, preferred) ===\n",
    "def calculate_discount(price: float, discount_pct: float) -> float:\n",
    "    \"\"\"Calculate discounted price.\"\"\"\n",
    "    breakpoint()  # Execution pauses here; opens pdb prompt\n",
    "    discount = price * (discount_pct / 100)\n",
    "    final_price = price - discount\n",
    "    return final_price\n",
    "\n",
    "# === Method 2: import pdb; pdb.set_trace() (older style) ===\n",
    "import pdb\n",
    "\n",
    "def process_data(items: list[int]) -> list[int]:\n",
    "    results = []\n",
    "    for item in items:\n",
    "        if item < 0:\n",
    "            pdb.set_trace()  # Stop here to investigate negative values\n",
    "        results.append(item * 2)\n",
    "    return results\n",
    "'''\n",
    "\n",
    "pdb_session_example = '''\n",
    "# === Example pdb session ===\n",
    "$ python my_script.py\n",
    "> /path/to/my_script.py(4)calculate_discount()\n",
    "-> discount = price * (discount_pct / 100)\n",
    "\n",
    "(Pdb) p price            # Print variable value\n",
    "100.0\n",
    "(Pdb) p discount_pct\n",
    "15.0\n",
    "(Pdb) n                  # Execute next line\n",
    "> /path/to/my_script.py(5)calculate_discount()\n",
    "-> final_price = price - discount\n",
    "(Pdb) p discount         # Check computed value\n",
    "15.0\n",
    "(Pdb) l                  # List source code\n",
    "  1     def calculate_discount(price: float, discount_pct: float) -> float:\n",
    "  2         breakpoint()\n",
    "  3         discount = price * (discount_pct / 100)\n",
    "  4  ->     final_price = price - discount\n",
    "  5         return final_price\n",
    "(Pdb) w                  # Where: show call stack\n",
    "  /path/to/my_script.py(8)<module>()\n",
    "-> result = calculate_discount(100.0, 15.0)\n",
    "> /path/to/my_script.py(5)calculate_discount()\n",
    "-> final_price = price - discount\n",
    "(Pdb) c                  # Continue execution\n",
    "'''\n",
    "\n",
    "print(\"=== pdb usage patterns ===\")\n",
    "print(pdb_example_basic)\n",
    "print(\"=== Example interactive session ===\")\n",
    "print(pdb_session_example)\n",
    "\n",
    "# Tip: disable all breakpoints without removing them\n",
    "print(\"=== Useful environment variables ===\")\n",
    "tips = {\n",
    "    \"PYTHONBREAKPOINT=0\": \"Disable all breakpoint() calls (for production)\",\n",
    "    \"PYTHONBREAKPOINT=ipdb.set_trace\": \"Use ipdb (enhanced debugger) instead of pdb\",\n",
    "    \"PYTHONBREAKPOINT=pudb.set_trace\": \"Use pudb (TUI debugger) instead of pdb\",\n",
    "}\n",
    "for var, desc in tips.items():\n",
    "    print(f\"  {var}\")\n",
    "    print(f\"    -> {desc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Assert Statements for Invariants\n",
    "\n",
    "`assert` statements verify conditions that should always be true.\n",
    "They serve as executable documentation and catch bugs early during\n",
    "development. Assertions can be globally disabled with `python -O`,\n",
    "so never use them for input validation or error handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BankAccount:\n",
    "    \"\"\"Bank account with assertion-guarded invariants.\"\"\"\n",
    "\n",
    "    owner: str\n",
    "    _balance: float = 0.0\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        assert isinstance(self.owner, str) and self.owner, \"Owner must be a non-empty string\"\n",
    "        assert self._balance >= 0, f\"Initial balance cannot be negative: {self._balance}\"\n",
    "\n",
    "    @property\n",
    "    def balance(self) -> float:\n",
    "        return self._balance\n",
    "\n",
    "    def deposit(self, amount: float) -> None:\n",
    "        # Precondition: amount must be positive\n",
    "        assert amount > 0, f\"Deposit amount must be positive, got {amount}\"\n",
    "\n",
    "        old_balance = self._balance\n",
    "        self._balance += amount\n",
    "\n",
    "        # Postcondition: balance increased by exactly the deposit amount\n",
    "        assert self._balance == old_balance + amount, \"Balance invariant violated\"\n",
    "\n",
    "    def withdraw(self, amount: float) -> None:\n",
    "        # Preconditions\n",
    "        assert amount > 0, f\"Withdrawal amount must be positive, got {amount}\"\n",
    "        assert amount <= self._balance, (\n",
    "            f\"Insufficient funds: requested {amount}, available {self._balance}\"\n",
    "        )\n",
    "\n",
    "        old_balance = self._balance\n",
    "        self._balance -= amount\n",
    "\n",
    "        # Postconditions\n",
    "        assert self._balance == old_balance - amount, \"Balance invariant violated\"\n",
    "        assert self._balance >= 0, \"Balance went negative -- this should be impossible\"\n",
    "\n",
    "\n",
    "# Normal usage\n",
    "print(\"=== Normal usage ===\")\n",
    "account = BankAccount(owner=\"Alice\", _balance=100.0)\n",
    "account.deposit(50.0)\n",
    "print(f\"  After deposit: ${account.balance:.2f}\")\n",
    "account.withdraw(30.0)\n",
    "print(f\"  After withdrawal: ${account.balance:.2f}\")\n",
    "\n",
    "# Precondition violations caught by assertions\n",
    "print(\"\\n=== Assertion violations ===\")\n",
    "\n",
    "violations: list[tuple[str, object]] = [\n",
    "    (\"Negative deposit\", lambda: account.deposit(-10)),\n",
    "    (\"Overdraft\", lambda: account.withdraw(999)),\n",
    "    (\"Empty owner\", lambda: BankAccount(owner=\"\", _balance=0)),\n",
    "    (\"Negative initial balance\", lambda: BankAccount(owner=\"Bob\", _balance=-50)),\n",
    "]\n",
    "\n",
    "for name, action in violations:\n",
    "    try:\n",
    "        action()  # type: ignore[operator]\n",
    "        print(f\"  {name}: No error (unexpected!)\")\n",
    "    except AssertionError as e:\n",
    "        print(f\"  {name}: AssertionError: {e}\")\n",
    "\n",
    "\n",
    "# Important: assert vs raise for validation\n",
    "print(\"\\n=== assert vs raise: when to use which ===\")\n",
    "guidelines = [\n",
    "    (\"assert\", \"Internal invariants, impossible states, developer errors\"),\n",
    "    (\"assert\", \"Can be disabled with python -O (optimized mode)\"),\n",
    "    (\"raise\", \"User input validation, expected error conditions\"),\n",
    "    (\"raise\", \"Always executes, even in optimized mode\"),\n",
    "]\n",
    "for keyword, description in guidelines:\n",
    "    print(f\"  {keyword:6s} -> {description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## timeit: Micro-Benchmarks\n",
    "\n",
    "The `timeit` module measures the execution time of small code snippets\n",
    "accurately, accounting for setup costs and running multiple iterations\n",
    "to reduce noise. This is the right tool for comparing alternative\n",
    "implementations of the same operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "\n",
    "# --- String concatenation: join vs += ---\n",
    "def build_string_concat(n: int) -> str:\n",
    "    \"\"\"Build a string using += concatenation.\"\"\"\n",
    "    result = \"\"\n",
    "    for i in range(n):\n",
    "        result += str(i) + \" \"\n",
    "    return result\n",
    "\n",
    "\n",
    "def build_string_join(n: int) -> str:\n",
    "    \"\"\"Build a string using str.join (preferred).\"\"\"\n",
    "    return \" \".join(str(i) for i in range(n))\n",
    "\n",
    "\n",
    "def build_string_fstring(n: int) -> str:\n",
    "    \"\"\"Build a string using list comprehension + join.\"\"\"\n",
    "    parts = [str(i) for i in range(n)]\n",
    "    return \" \".join(parts)\n",
    "\n",
    "\n",
    "# timeit.timeit runs the statement many times and returns total seconds\n",
    "n = 1000\n",
    "iterations = 500\n",
    "\n",
    "print(f\"=== String building ({n} items, {iterations} iterations) ===\")\n",
    "\n",
    "t_concat = timeit.timeit(lambda: build_string_concat(n), number=iterations)\n",
    "t_join = timeit.timeit(lambda: build_string_join(n), number=iterations)\n",
    "t_listcomp = timeit.timeit(lambda: build_string_fstring(n), number=iterations)\n",
    "\n",
    "fastest = min(t_concat, t_join, t_listcomp)\n",
    "print(f\"  += concat:     {t_concat:.4f}s  ({t_concat/fastest:.1f}x)\")\n",
    "print(f\"  join(genexpr): {t_join:.4f}s  ({t_join/fastest:.1f}x)\")\n",
    "print(f\"  join(listcomp):{t_listcomp:.4f}s  ({t_listcomp/fastest:.1f}x)\")\n",
    "\n",
    "\n",
    "# --- List creation: append vs comprehension ---\n",
    "print(f\"\\n=== List creation ({n} items, {iterations} iterations) ===\")\n",
    "\n",
    "t_append = timeit.timeit(\n",
    "    lambda: [None for _ in range(n)] or [(lst := []) or [lst.append(i * 2) for i in range(n)]] and lst,\n",
    "    number=iterations,\n",
    ")\n",
    "\n",
    "def build_list_append(n: int) -> list[int]:\n",
    "    result: list[int] = []\n",
    "    for i in range(n):\n",
    "        result.append(i * 2)\n",
    "    return result\n",
    "\n",
    "def build_list_comprehension(n: int) -> list[int]:\n",
    "    return [i * 2 for i in range(n)]\n",
    "\n",
    "t_append = timeit.timeit(lambda: build_list_append(n), number=iterations)\n",
    "t_comp = timeit.timeit(lambda: build_list_comprehension(n), number=iterations)\n",
    "\n",
    "fastest = min(t_append, t_comp)\n",
    "print(f\"  list.append(): {t_append:.4f}s  ({t_append/fastest:.1f}x)\")\n",
    "print(f\"  list comp:     {t_comp:.4f}s  ({t_comp/fastest:.1f}x)\")\n",
    "\n",
    "\n",
    "# --- Dictionary lookup vs if/elif chain ---\n",
    "print(f\"\\n=== Dispatch: dict lookup vs if/elif ===\")\n",
    "\n",
    "def dispatch_if(op: str, a: int, b: int) -> int:\n",
    "    if op == \"add\":\n",
    "        return a + b\n",
    "    elif op == \"sub\":\n",
    "        return a - b\n",
    "    elif op == \"mul\":\n",
    "        return a * b\n",
    "    return 0\n",
    "\n",
    "dispatch_table = {\n",
    "    \"add\": lambda a, b: a + b,\n",
    "    \"sub\": lambda a, b: a - b,\n",
    "    \"mul\": lambda a, b: a * b,\n",
    "}\n",
    "\n",
    "def dispatch_dict(op: str, a: int, b: int) -> int:\n",
    "    return dispatch_table[op](a, b)\n",
    "\n",
    "iters = 100_000\n",
    "t_if = timeit.timeit(lambda: dispatch_if(\"mul\", 10, 20), number=iters)\n",
    "t_dict = timeit.timeit(lambda: dispatch_dict(\"mul\", 10, 20), number=iters)\n",
    "\n",
    "fastest = min(t_if, t_dict)\n",
    "print(f\"  if/elif:    {t_if:.4f}s  ({t_if/fastest:.1f}x)\")\n",
    "print(f\"  dict lookup:{t_dict:.4f}s  ({t_dict/fastest:.1f}x)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## cProfile: Function-Level Profiling\n",
    "\n",
    "`cProfile` profiles an entire program and reports time spent in each\n",
    "function call. This is the right tool for finding bottlenecks in\n",
    "real applications. Key columns in the output:\n",
    "\n",
    "- **ncalls**: Number of times the function was called\n",
    "- **tottime**: Total time spent IN the function (excluding sub-calls)\n",
    "- **cumtime**: Cumulative time (including sub-calls)\n",
    "- **percall**: Time per call (tottime/ncalls or cumtime/ncalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile\n",
    "import pstats\n",
    "import io\n",
    "\n",
    "\n",
    "def simulate_data_pipeline() -> dict[str, object]:\n",
    "    \"\"\"A multi-step pipeline to profile.\"\"\"\n",
    "    raw_data = generate_data(10_000)\n",
    "    cleaned = clean_data(raw_data)\n",
    "    transformed = transform_data(cleaned)\n",
    "    summary = summarize_data(transformed)\n",
    "    return summary\n",
    "\n",
    "\n",
    "def generate_data(n: int) -> list[dict[str, object]]:\n",
    "    \"\"\"Generate synthetic data records.\"\"\"\n",
    "    return [\n",
    "        {\"id\": i, \"value\": i * 1.5, \"category\": f\"cat_{i % 5}\", \"valid\": i % 7 != 0}\n",
    "        for i in range(n)\n",
    "    ]\n",
    "\n",
    "\n",
    "def clean_data(records: list[dict[str, object]]) -> list[dict[str, object]]:\n",
    "    \"\"\"Filter out invalid records.\"\"\"\n",
    "    return [r for r in records if r[\"valid\"]]\n",
    "\n",
    "\n",
    "def transform_data(records: list[dict[str, object]]) -> list[dict[str, object]]:\n",
    "    \"\"\"Apply transformations to each record.\"\"\"\n",
    "    results = []\n",
    "    for record in records:\n",
    "        transformed = {\n",
    "            \"id\": record[\"id\"],\n",
    "            \"value_squared\": float(record[\"value\"]) ** 2,  # type: ignore[arg-type]\n",
    "            \"category\": str(record[\"category\"]).upper(),\n",
    "        }\n",
    "        results.append(transformed)\n",
    "    return results\n",
    "\n",
    "\n",
    "def summarize_data(records: list[dict[str, object]]) -> dict[str, object]:\n",
    "    \"\"\"Compute summary statistics.\"\"\"\n",
    "    values = [float(r[\"value_squared\"]) for r in records]  # type: ignore[arg-type]\n",
    "    categories: dict[str, int] = {}\n",
    "    for r in records:\n",
    "        cat = str(r[\"category\"])\n",
    "        categories[cat] = categories.get(cat, 0) + 1\n",
    "\n",
    "    return {\n",
    "        \"count\": len(records),\n",
    "        \"mean_value_sq\": sum(values) / len(values) if values else 0,\n",
    "        \"max_value_sq\": max(values) if values else 0,\n",
    "        \"categories\": categories,\n",
    "    }\n",
    "\n",
    "\n",
    "# Profile the pipeline\n",
    "print(\"=== cProfile output (sorted by cumulative time) ===\")\n",
    "profiler = cProfile.Profile()\n",
    "profiler.enable()\n",
    "result = simulate_data_pipeline()\n",
    "profiler.disable()\n",
    "\n",
    "# Format the output\n",
    "stream = io.StringIO()\n",
    "stats = pstats.Stats(profiler, stream=stream)\n",
    "stats.sort_stats(\"cumulative\")\n",
    "stats.print_stats(15)  # Top 15 functions\n",
    "print(stream.getvalue())\n",
    "\n",
    "print(f\"Pipeline result: {result['count']} records, mean={result['mean_value_sq']:.1f}\")\n",
    "\n",
    "# You can also sort by different criteria\n",
    "print(\"\\n=== Top 5 by total time (excluding sub-calls) ===\")\n",
    "stream = io.StringIO()\n",
    "stats = pstats.Stats(profiler, stream=stream)\n",
    "stats.sort_stats(\"tottime\")\n",
    "stats.print_stats(5)\n",
    "print(stream.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Memory Considerations\n",
    "\n",
    "Python objects carry overhead beyond their raw data. `sys.getsizeof` shows\n",
    "the shallow memory size of an object. For classes, `__slots__` eliminates\n",
    "the per-instance `__dict__`, significantly reducing memory for many instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "# --- Object sizes ---\n",
    "print(\"=== Object memory sizes (sys.getsizeof) ===\")\n",
    "objects: list[tuple[str, object]] = [\n",
    "    (\"int(0)\", 0),\n",
    "    (\"int(1)\", 1),\n",
    "    (\"int(2**30)\", 2**30),\n",
    "    (\"float(1.0)\", 1.0),\n",
    "    (\"bool(True)\", True),\n",
    "    (\"None\", None),\n",
    "    (\"'' (empty str)\", \"\"),\n",
    "    (\"'hello'\", \"hello\"),\n",
    "    (\"'x' * 100\", \"x\" * 100),\n",
    "    (\"[] (empty list)\", []),\n",
    "    (\"list(range(10))\", list(range(10))),\n",
    "    (\"list(range(100))\", list(range(100))),\n",
    "    (\"{} (empty dict)\", {}),\n",
    "    (\"dict 10 items\", {i: i for i in range(10)}),\n",
    "    (\"set() empty\", set()),\n",
    "    (\"tuple() empty\", ()),\n",
    "    (\"tuple(range(10))\", tuple(range(10))),\n",
    "]\n",
    "\n",
    "for label, obj in objects:\n",
    "    print(f\"  {label:25s} -> {sys.getsizeof(obj):>6} bytes\")\n",
    "\n",
    "\n",
    "# --- __slots__ vs regular classes ---\n",
    "print(\"\\n=== __slots__ memory savings ===\")\n",
    "\n",
    "\n",
    "class PointRegular:\n",
    "    \"\"\"Regular class with __dict__.\"\"\"\n",
    "    def __init__(self, x: float, y: float, z: float) -> None:\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.z = z\n",
    "\n",
    "\n",
    "class PointSlots:\n",
    "    \"\"\"Slotted class: no __dict__, fixed attributes.\"\"\"\n",
    "    __slots__ = (\"x\", \"y\", \"z\")\n",
    "\n",
    "    def __init__(self, x: float, y: float, z: float) -> None:\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.z = z\n",
    "\n",
    "\n",
    "regular = PointRegular(1.0, 2.0, 3.0)\n",
    "slotted = PointSlots(1.0, 2.0, 3.0)\n",
    "\n",
    "regular_size = sys.getsizeof(regular) + sys.getsizeof(regular.__dict__)\n",
    "slotted_size = sys.getsizeof(slotted)\n",
    "\n",
    "print(f\"  Regular (instance + __dict__): {regular_size} bytes\")\n",
    "print(f\"  Slotted (instance only):       {slotted_size} bytes\")\n",
    "print(f\"  Savings per instance:          {regular_size - slotted_size} bytes ({(1 - slotted_size/regular_size)*100:.0f}%)\")\n",
    "\n",
    "# Scale it up: creating many instances\n",
    "N = 100_000\n",
    "print(f\"\\n  For {N:,} instances:\")\n",
    "print(f\"    Regular: ~{regular_size * N / 1024 / 1024:.1f} MB\")\n",
    "print(f\"    Slotted: ~{slotted_size * N / 1024 / 1024:.1f} MB\")\n",
    "\n",
    "# Slots limitation: cannot add arbitrary attributes\n",
    "print(\"\\n  Slots limitation:\")\n",
    "try:\n",
    "    slotted.w = 4.0  # type: ignore[attr-defined]\n",
    "except AttributeError as e:\n",
    "    print(f\"    Cannot add attribute: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Common Performance Patterns\n",
    "\n",
    "Python has well-known performance characteristics. Understanding these\n",
    "patterns helps you write efficient code without premature optimization.\n",
    "\n",
    "Key patterns:\n",
    "1. **Local variable lookup** is faster than global or attribute lookup\n",
    "2. **`str.join()`** is faster than `+=` for building strings\n",
    "3. **Generators** use constant memory; lists use O(n) memory\n",
    "4. **List comprehensions** are faster than manual `append` loops\n",
    "5. **`in` on sets/dicts** is O(1); `in` on lists is O(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "import sys\n",
    "\n",
    "\n",
    "# --- Local vs global vs attribute lookup ---\n",
    "print(\"=== Variable lookup speed ===\")\n",
    "\n",
    "GLOBAL_VALUE = 42\n",
    "\n",
    "class Container:\n",
    "    value = 42\n",
    "\n",
    "container = Container()\n",
    "\n",
    "def lookup_global() -> int:\n",
    "    total = 0\n",
    "    for _ in range(1000):\n",
    "        total += GLOBAL_VALUE\n",
    "    return total\n",
    "\n",
    "def lookup_local() -> int:\n",
    "    local_value = 42\n",
    "    total = 0\n",
    "    for _ in range(1000):\n",
    "        total += local_value\n",
    "    return total\n",
    "\n",
    "def lookup_attribute() -> int:\n",
    "    total = 0\n",
    "    for _ in range(1000):\n",
    "        total += container.value\n",
    "    return total\n",
    "\n",
    "iters = 2000\n",
    "t_global = timeit.timeit(lookup_global, number=iters)\n",
    "t_local = timeit.timeit(lookup_local, number=iters)\n",
    "t_attr = timeit.timeit(lookup_attribute, number=iters)\n",
    "\n",
    "fastest = min(t_global, t_local, t_attr)\n",
    "print(f\"  Global:    {t_global:.4f}s  ({t_global/fastest:.2f}x)\")\n",
    "print(f\"  Local:     {t_local:.4f}s  ({t_local/fastest:.2f}x)\")\n",
    "print(f\"  Attribute: {t_attr:.4f}s  ({t_attr/fastest:.2f}x)\")\n",
    "\n",
    "\n",
    "# --- Membership testing: list vs set ---\n",
    "print(\"\\n=== Membership testing: list O(n) vs set O(1) ===\")\n",
    "\n",
    "data_list = list(range(10_000))\n",
    "data_set = set(data_list)\n",
    "search_values = [5000, 9999, 10_001]  # Middle, end, missing\n",
    "\n",
    "for target in search_values:\n",
    "    iters = 5000\n",
    "    t_list = timeit.timeit(lambda: target in data_list, number=iters)\n",
    "    t_set = timeit.timeit(lambda: target in data_set, number=iters)\n",
    "    print(f\"  Search for {target:>6}: list={t_list:.4f}s, set={t_set:.4f}s, ratio={t_list/t_set:.0f}x\")\n",
    "\n",
    "\n",
    "# --- Generator vs list: memory comparison ---\n",
    "print(\"\\n=== Generator vs list memory ===\")\n",
    "\n",
    "n = 1_000_000\n",
    "\n",
    "# List stores all values in memory\n",
    "large_list = [i * 2 for i in range(n)]\n",
    "list_size = sys.getsizeof(large_list)\n",
    "\n",
    "# Generator computes values on demand\n",
    "large_gen = (i * 2 for i in range(n))\n",
    "gen_size = sys.getsizeof(large_gen)\n",
    "\n",
    "print(f\"  List of {n:,} items: {list_size:>12,} bytes ({list_size / 1024 / 1024:.1f} MB)\")\n",
    "print(f\"  Generator:         {gen_size:>12,} bytes\")\n",
    "print(f\"  Memory ratio: {list_size / gen_size:,.0f}x\")\n",
    "\n",
    "# Both produce the same sum\n",
    "print(f\"\\n  sum(list) == sum(generator): {sum(large_list) == sum(i * 2 for i in range(n))}\")\n",
    "\n",
    "\n",
    "# --- Practical tip: use generators in pipelines ---\n",
    "print(\"\\n=== Generator pipeline (constant memory) ===\")\n",
    "\n",
    "def read_records(n: int):\n",
    "    \"\"\"Simulate reading records from a large file.\"\"\"\n",
    "    for i in range(n):\n",
    "        yield {\"id\": i, \"value\": i * 1.5, \"active\": i % 3 != 0}\n",
    "\n",
    "def filter_active(records):\n",
    "    \"\"\"Filter to active records only.\"\"\"\n",
    "    for record in records:\n",
    "        if record[\"active\"]:\n",
    "            yield record\n",
    "\n",
    "def extract_values(records):\n",
    "    \"\"\"Extract just the values.\"\"\"\n",
    "    for record in records:\n",
    "        yield record[\"value\"]\n",
    "\n",
    "# This pipeline processes 1M records in constant memory\n",
    "pipeline = extract_values(filter_active(read_records(1_000_000)))\n",
    "total = sum(pipeline)  # Only one record in memory at a time\n",
    "print(f\"  Sum of 1M filtered records: {total:,.1f}\")\n",
    "print(f\"  Pipeline generator size: {sys.getsizeof(pipeline)} bytes (constant!)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}